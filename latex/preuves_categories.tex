%\input{preamble}
%\begin{document}
%\renewcommand{\labelitemi}{$\bullet$}
%\vspace{2cm}~~\\
%\input{bussproofs}



%communs : id,coupure
\def\id{
\centerAlignProof
	\AXC	{}
	\RightLabel{$(id)$}
	\UIC{$A \vdash A$}
	\DP
}
\def\cut{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G',A \vdash \D'$}
	\RightLabel{$(cut)$}
	\BIC{$\G,\G' \vdash \D,\D'$}
	\DP
}



\part*{Mémoire}

\idees{Rapport de stage : page \pageref{part:rapportstage}}

\section*{Introduction}


\tableofcontents

\

\

\idees{mettre le plan du rapport de stage au début du rapport de stage}


%%%

\section{Introduction aux calculs de séquents à travers \LK\ pour la logique classique et \LJ\ pour la logique intuitionniste}
\label{section:introductionCalculsSequents}



\idees{
calculs de séquents :
outils / procédé de raisonnement
... dans l'introduction générale du mémoire ?
}

Nous introduisons dans cette première partie les définitions dont nous aurons besoin sur les calculs de séquents, à travers l'exemple du calcul de séquents \LK. Nous expliquons en quoi ce calcul défini par Gentzen correspond à la logique classique. Nous présentons ensuite le calcul \LJ, que Gentzen a dérivé de \LK\ afin de représenter la logique intuitionniste. Enfin, nous expliquons comment certaines propriétés des deux logiques considérées peuvent se comprendre en étudiant leur calcul de séquents respectif.

Les formules considérées dans cette partie sont construites à partir de constantes $\bot$ (\emph{faux}) et $\top$ (\emph{vrai}), de variables propositionnelles, du connecteur unaire $\lnot$ (\emph{non}), et des connecteurs binaires $\land$ (\emph{et}), $\lor$ (\emph{ou}) et $\to$ (\emph{implique}). On s'intéresse en effet à la partie propositionnelle de chaque logique.


\subsection{Des séquents et des règles}

Un calcul de séquents se caractérise par sa propre définition d'un objet syntaxique appelé \emph{séquent}, ainsi que par un ensemble de \emph{règles} agissant sur les séquents.

Par exemple, pour le calcul de séquents \LK, la définition d'un séquent est la suivante, et les règles sont données dans la figure~\ref{fig:reglesLK}.

\begin{df}
Un \textbf{\emph{séquent}} de \LK\ consiste en deux listes de formules $\G$ (les ``hypothèses'') et $\D$ (les ``conclusions'') ; on l'écrit $\G \vdash \D$.
\end{df}


\noindent
\textit{Notation.} $X,Y$ désigne la liste obtenue en concaténant les listes $X$ et $Y$ ; si $X$ ou $Y$ est une formule, on la considère comme la liste à un élément correspondante.

%LK
%id, coupure : communs

%LK,règles logiques
\def\LKfauxL{
	\AXC	{}
	\RightLabel{$(\bot L)$}
	\UIC{$\G,\bot \vdash \D$}
	\DP
}
\def\LKvraiR{
	\AXC	{}
	\RightLabel{$(\top R)$}
	\UIC{$\G \vdash \top,\D$}
	\DP
}
\def\LKnonL{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(\lnot L)$}
	\UIC{$\G,\lnot A \vdash \D$}
	\DP
}
\def\LKnonR{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(\lnot L)$}
	\UIC{$\G \vdash \lnot A,\D$}
	\DP
}
\def\LKetL{
	\AXC	{$\G,A,B \vdash \D$}
	\RightLabel{$(\land L)$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP
}
\def\LKetR{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G \vdash B,\D$}
	\RightLabel{$(\land R)$}
	\BIC{$\G \vdash A\land B,\D$}
	\DP
}
\def\LKouL{
	\AXC	{$\G,A \vdash \D$}
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\lor L)$}
	\BIC{$\G,A\lor B \vdash \D$}
	\DP
}
\def\LKouR{
	\AXC	{$\G \vdash A,B,\D$}
	\RightLabel{$(\lor R)$}
	\UIC{$\G \vdash A\lor B,\D$}
	\DP
}
\def\LKimpL{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\to L)$}
	\BIC{$\G,A\to B \vdash \D$}
	\DP
}
\def\LKimpR{
	\AXC	{$\G,A \vdash B,\D$}
	\RightLabel{$(\to R)$}
	\UIC{$\G \vdash A\to B,\D$}
	\DP
}
%LK, règles structurelles
\def\LKweakeningL{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(weakening\ L)$}
	\UIC{$\G,A \vdash \D$}
	\DP
}
\def\LKweakeningR{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(weakening\ R)$}
	\UIC{$\G \vdash A,\D$}
	\DP
}
\def\LKcontractionL{
	\AXC	{$\G,A,A \vdash \D$}
	\RightLabel{$(contraction\ L)$}
	\UIC{$\G,A \vdash \D$}
	\DP
}
\def\LKcontractionR{
	\AXC	{$\G \vdash A,A,\D$}
	\RightLabel{$(contraction\ R)$}
	\UIC{$\G \vdash A,\D$}
	\DP
}
\def\LKexchangeL{
	\AXC	{$\G_{1},A,B,\G_{2} \vdash \D$}
	\RightLabel{$(exchange\ L)$}
	\UIC{$\G_{1},B,A,\G_{2} \vdash \D$}
	\DP
}
\def\LKexchangeR{
	\AXC	{$\G \vdash \D_{1},A,B,\D_{2}$}
	\RightLabel{$(exchange\ L)$}
	\UIC{$\G \vdash \D_{1},B,A,\D_{2}$}
	\DP
}

%LK variantes
\def\LKetLun{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(\land L_{1})$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP
}
\def\LKetLdeux{
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\land L_{2})$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP
}

\def\LKouRun{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(\lor R_{1})$}
	\UIC{$\G \vdash A\lor B,\D$}
	\DP
}
\def\LKouRdeux{
	\AXC	{$\G \vdash B,\D$}
	\RightLabel{$(\lor R_{2})$}
	\UIC{$\G \vdash A\lor B,\D$}
	\DP
}


\begin{figure}[h]
%\begin{floatingfigure}[r]{0.6\linewidth}
\centering
{\renewcommand{\arraystretch}{1.3}
\resizebox{0.6\linewidth}{!}{
%$\begin{tabular}{|cc|c|}
%	\hline
%	\textbf{\large{Identité}} & \LJid & \textbf{\large{Coupure}} \\
%	\cline{1-2}
%	\LJbotL & \textbf{\large{Règles logiques}} & \LJcut \\[4pt]
%	\cline{3-3}
%	\LJetL & \LJetR & \textbf{\large{Règles structurelles}} \\
%	\LJouL & \LJouRun \LJouRdeux & \LJweakening \\
%	\LJimpL & \LJimpR & \LJcontraction \\[4pt]
%	\hline
%\end{tabular}$
$\begin{tabular}{|cc|}
	\hline
	\multicolumn{1}{|c|}{\textbf{\large{Identité}}} & \textbf{\large{Coupure}} \\ 
	\multicolumn{1}{|c|}{\id} & \cut \\[9pt] \hline
	\multicolumn{2}{|c|}{\textbf{\large{Règles logiques}}} \\
	\LKfauxL & \LKvraiR \\[5pt]
	\LKnonL & \LKnonR \\[9pt]
	\LKetL & \LKetR \\[9pt]
	\LKouL & \LKouR \\[9pt]
	\LKimpL & \LKimpR \\[9pt] \hline
	\multicolumn{2}{|c|}{\textbf{\large{Règles structurelles}}} \\
	\LKweakeningL & \LKweakeningR \\[9pt]
	\LKcontractionL & \LKcontractionR \\[9pt]
	\LKexchangeL & \LKexchangeR \\[9pt] \hline
%	\LKL & \LKR \\
\end{tabular}$
}
}
\caption{Règles du calcul \LK}
\label{fig:reglesLK}
\end{figure}
%\end{floatingfigure}


Pour une règle ${\regle,}$ $\mathcal R$ est le nom de la règle, $prem_{1}$, ... , $prem_{p}$ sont les \textbf{\emph{prémisses}}, et $concl$ la \textbf{\emph{conclusion}}. 
%$prem_{k}$ sera appelée la \emph{$k$-ième prémisse} ou \emph{prémisse numéro $k$}.%?
Les prémisses et la conclusion sont des séquents où $A$, $B$ sont des formules quelconques et $\G$, $\D$ des listes de formules quelconques. 
L'idée est qu'une règle affirme : si toutes les prémisses sont vraies, alors la conclusion est aussi vraie. Cette idée est formalisée en \ref{subsection:ProuvabiliteSequent}.
%Les \textbf{\emph{axiomes}} sont les règles sans prémisse.%?

On distingue deux grandes familles de règles. Les \textbf{\emph{règles logiques}} remplacent une formule de la conclusion par une ou des formules plus simples. La formule remplacée, appelée \emph{formule principale}, doit avoir une forme donnée en fonction de la règle. Les  \textbf{\emph{règles structurelles}} manipulent la structure du séquent en enlevant, dupliquant, dépla\c cant des formules dont on n'a pas besoin de connaître la forme. Elles dépendent du choix de structure du séquent : 
par exemple, si on représentait $\G$ et $\D$ par des \emph{multiensembles}, c'est-à-dire des collections où le nombre d'occurrences est pris en compte mais pas l'ordre des éléments, on n'aurait pas besoin des règles d'échange $exchange\ L$ et $exchange\ R$.

\subsection{Variantes d'écriture de certaines règles}
\label{subsection:VariantesEcritureRegles}

Cette présentation %, inspirée de la présentation de \LJ\ par Dyckhoff dans \cite{LJT}, 
diffère %légèrement 
de celle de Gentzen, mais elle en est suffisamment proche pour qu'on puisse quand même appeler ce calcul de séquents \LK. Gentzen écrit deux règles\LKetLun et \LKetLdeux à la place de \LKetL, et de même deux autres règles à la place de $\lor R$. On a cependant une équivalence grâce aux règles structurelles. Ci-dessous à gauche, on retrouve en effet la règle $\land L_{1}$ à partir de $\land L$ et $weakening\ L$, et on peut faire de même pour $\land L_{2}$. \`A droite, on retrouve $\land L$ à partir de $\land L_{1}$ et $\land L_{2}$ et $contraction\ L$. On s'autorise à faire agir $\land L_{1}$ sur une formule qui n'est pas la dernière de la liste : cela est possible en appliquant plusieurs fois la règle $exchange\ L$, 
ce qu'on a pas fait explicitement par souci de lisibilité.
%mais le faire explicitement alourdirait 
\vspace{1mm}

{
\centering
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(weakening L)$}
	\UIC{$\G,A,B \vdash \D$}
	\RightLabel{$(\land L)$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP
\qquad
	\AXC	{$\G,A,B \vdash \D$}
	\RightLabel{$(\land L_{1})$}
	\UIC{$\G,A\land B,B \vdash \D$}
	\RightLabel{$(\land L_{2})$}
	\UIC{$\G,A\land B,A\land B \vdash \D$}
	\RightLabel{$(contraction L)$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP

}
%	\AXC	{$\G,A \vdash \D$}
%	\RightLabel{$(\land L_{1})$}
%	\UIC{$\G,A\land B \vdash \D$}
%	\DP




\subsection{Prouvabilité d'un séquent}
\label{subsection:ProuvabiliteSequent}

%Les définitions suivantes s'appliquent aux calculs de séquents en général, pas seulement \LJ.
Une \textbf{\emph{instance}} d'une règle $\mathcal R$ a la même forme que la règle : \instance, mais ici les $\s_{i}$ et $\s$ sont des séquents connus explicitement ; bien entendu il faut qu'il s'agisse de séquents qui correspondent à la forme donnée par la définition de la règle. %Par exemple \etL\ devient une instance de la règle $\land L$ (qui a la même écriture que la règle) lorsqu'on connaît les formules $A$ et $B$ et toutes les formules de $\Th$, $\G$, $\D$.
Une \textbf{\emph{preuve}} (ou \textbf{\emph{arbre de preuve}}) est un arbre dont les n\oe uds sont étiquetés par un séquent et une règle et ont la même arité que le nombre de prémisses de la règle, et tel que : pour tout n\oe ud de séquent $\s$ et de règle $\mathcal R$, si $\s_{1}$, ... , $\s_{p}$ sont les séquents associés à chacun de ses fils respectivement, alors\instance\ est une instance de $\mathcal R$. Les feuilles d'un tel arbre sont les n\oe uds auxquels est associé un axiome.

\begin{df}
Un séquent $\s$ est \textbf{\emph{prouvable}} \emph{dans} 
%(ou \emph{par}) 
%ou \emph{par}
\emph{un calcul de séquents} s'il existe un arbre de preuve tel que le séquent associé à la racine est $\s$. De manière équivalente, on peut définir l'ensemble des séquents prouvables comme le plus petit ensemble vérifiant : pour toute instance \instance\ d'une règle, si pour tout $i$, $\s_{i}$ est prouvable, alors $\s$ est prouvable (en particulier pour toute instance \instanceAx\ d'un axiome $\mathcal A$, $\s$ est prouvable).
\end{df}


\subsection{Lien avec une logique, interprétation des séquents}

Un calcul de séquents est généralement associé à une logique, à travers une propriété similaire à la suivante, qui concerne \LK\ et la logique classique.

\begin{prop}
Une formule $A$ est valide en logique classique si, et seulement si, le séquent $\;\vdash A$ est prouvable par le calcul \LK\ (on écrit $\;\vdash A$ pour $\emptyset \vdash A$, $\emptyset$ désignant ici la liste vide).
\end{prop}

Les séquents sont des objets syntaxiques pratiques à manipuler à l'aide de règles. Cependant, ils ont souvent une interprétation dans la logique considérée : par exemple pour \LK, un séquent $\G \vdash \D$ s'interprète comme une formule de logique classique grâce à la propriété suivante. Il signifie ainsi : ``si on suppose toutes les formules de $\G$, on peut montrer au moins une formule de $\D$'', d'où
%De là viennent 
les appellations ``hypothèses'' pour les formules de $\G$ et ``conclusions'' pour celles de $\D$.

\begin{prop}
Un séquent $\G \vdash \D$ est prouvable par le calcul \LK\ si, et seulement si, la formule $\left(\bigwedge_{G\in\G}G\right)\to\left(\bigvee_{D\in\D}D\right)$ est valide en logique classique.
\end{prop}

\subsection{Calcul des séquents \LJ\ et logique intuitionniste}

Gentzen a dérivé le calcul \LJ\ de \LK\ dans le but de correspondre à la logique intuitionniste. La modification apportée à \LK\ pour cela est simple et efficace : on restreint les séquents à ceux qui ont exactement une formule à droite, c'est-à-dire dans $\D$ avec les notations habituelles ; les règles sont adaptées en conséquence. La principale conséquence est l'impossibilité d'utiliser les règles structurelles à droite du séquent. La nouvelle définition d'un séquent est donc la suivante, et les règles sont données dans la figure~\ref{fig:reglesLJ}. 

\begin{df}
Un \textbf{\emph{séquent}} de \LJ\ consiste en une liste de formules $\G$ (les ``hypothèses'') et une formule $D$ (la ``conclusion'') ; on l'écrit $\G \vdash D$.
\end{df}


%LJ
%id : commun
%cut
\def\LJcut{
	\AXC	{$\G \vdash A$}
	\AXC	{$\G',A \vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$\G,\G' \vdash D$}
	\DP
}

%LJ,règles logiques
\def\LJfauxL{
	\AXC	{}
	\RightLabel{$(\bot L)$}
	\UIC{$\G,\bot \vdash D$}
	\DP
}
\def\LJetL{
	\AXC	{$\G,A,B \vdash D$}
	\RightLabel{$(\land L)$}
	\UIC{$\G,A\land B \vdash D$}
	\DP
}
\def\LJetR{
	\AXC	{$\G \vdash A$}
	\AXC	{$\G \vdash B$}
	\RightLabel{$(\land R)$}
	\BIC{$\G \vdash A\land B$}
	\DP
}
\def\LJouL{
	\AXC	{$\G,A \vdash D$}
	\AXC	{$\G,B \vdash D$}
	\RightLabel{$(\lor L)$}
	\BIC{$\G,A\lor B \vdash D$}
	\DP
}
\def\LJouRun{
	\AXC	{$\G \vdash A$}
	\RightLabel{$(\lor R_{1})$}
	\UIC{$\G \vdash A\lor B$}
	\DP
}
\def\LJouRdeux{
	\AXC	{$\G \vdash B$}
	\RightLabel{$(\lor R_{2})$}
	\UIC{$\G \vdash A\lor B$}
	\DP
}
\def\LJimpL{
	\AXC	{$\G \vdash A$}
	\AXC	{$\G,B \vdash D$}
	\RightLabel{$(\to L)$}
	\BIC{$\G,A\to B \vdash D$}
	\DP
}
\def\LJimpR{
	\AXC	{$\G,A \vdash B$}
	\RightLabel{$(\to R)$}
	\UIC{$\G \vdash A\to B$}
	\DP
}
%LJ, règles structurelles
\def\LJcontractionL{
	\AXC	{$\G,A,A \vdash D$}
	\RightLabel{$(contraction\ L)$}
	\UIC{$\G,A \vdash D$}
	\DP
}
\def\LJweakeningL{
	\AXC	{$\G \vdash D$}
	\RightLabel{$(weakening\ L)$}
	\UIC{$\G,A \vdash D$}
	\DP
}
\def\LJexchangeL{
	\AXC	{$\G_{1},A,B,\G_{2} \vdash D$}
	\RightLabel{$(exchange\ L)$}
	\UIC{$\G_{1},B,A,\G_{2} \vdash D$}
	\DP
}


\begin{figure}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\resizebox{0.7\linewidth}{!}{
$\begin{tabular}{|cc|}
	\hline 
	\multicolumn{1}{|c|}{\textbf{\large{Identité}}} & \textbf{\large{Coupure}} \\
	\multicolumn{1}{|c|}{\id} & \LJcut \\[7pt] \hline
	\LJfauxL & \textbf{\large{Règles logiques}} \\[5pt]
	\LJetL & \LJetR \\[9pt]
	\LJouL & \LJouRun \LJouRdeux \\[9pt]
	\LJimpL & \LJimpR \\[7pt] \hline
%	\multicolumn{2}{|c|}{\textbf{\large{Règles structurelles}}} \\ \hline
	\textbf{\large{Règles structurelles}} & \LJweakeningL \\[9pt]
	\LJcontractionL & \LJexchangeL \\[7pt] \hline
\end{tabular}$
}
\caption{Règles du calcul \LJ}
\label{fig:reglesLJ}
\end{figure}


Les règles structurelles à droite de \LK, qui changent le nombre de formules à droite ou en nécessitent au moins deux, disparaissent. La règle\LKouL ne s'adapte pas non plus aux séquents de \LJ\ ; on la remplace par deux règles comme en \ref{subsection:VariantesEcritureRegles}.

Le connecteur $\lnot$ disparaît également. En logique intuitionniste, $\lnot A$ a bien un sens, mais on préfère considérer qu'il s'agit d'une simple notation pour $A\!\to\!\bot$. Cette décision est justifiée par le fait qu'en logique intuitionniste, $\lnot$ n'est pas involutif : la formule $\lnot\lnot A$ n'est pas équivalente à $A$.

\

Comme \LK\ avec la logique classique, \LJ\ est liée à la logique intuitionniste : \LJ\ permet de caractériser la prouvabilité d'une formule en logique intuitionniste. Un séquent de \LJ\ peut également s'interpréter en logique intuitionniste.

\begin{prop}
Une formule $A$ est prouvable en logique intuitionniste si, et seulement si, le séquent $\;\vdash A$ est prouvable par le calcul \LJ.
\end{prop}

\begin{prop}
Un séquent $\G \vdash D$ est prouvable par le calcul \LJ\ si, et seulement si, la formule $\left(\bigwedge_{G\in\G}G\right)\to D$ est prouvable en logique intuitionniste.
\end{prop}


%

\subsection{Une différence entre logique classique et logique intuitionniste}

\begin{floatingfigure}[r]{4.2cm}
\centering
	\AXC	{}
	\RightLabel{(id)}
	\UIC	{$A \vdash A$}
	\RightLabel{(weak.\ R)}
	\UIC	{$A \vdash A, \bot$}
	\RightLabel{$(\to R)$}
	\UIC	{$\vdash A\,,\; A\!\to\!\bot$}
	\RightLabel{$(\lor R)$}
	\UIC{$\vdash A\lor (A\!\to\!\bot)$}
	\DP
\\
%\caption{Preuve de $A\lor(A\!\to\!\bot)$ dans \LK}
%\label{fig:LKTiersExclu}
\end{floatingfigure}

C'est la possibilité d'avoir plusieurs formules dans la partie droite du séquent qui permet de prouver davantage de séquents dans \LK\ que dans \LJ. On comprend ainsi la différence entre le ``ou'' classique et le ``ou'' intuitionniste. En logique classique, prouver $A\lor B$, c'est prouver le séquent $\;\vdash A,B$ : les deux formules sont encore présentes. Un bon exemple est la preuve ci-contre dans \LK\ du principe du tiers exclu $A\lor\lnot A$ 
%(figure~\ref{fig:LKTiersExclu} ; on rappelle que $\lnot A$ est une notation pour $A\to\bot$) : 
, qu'on écrit $A\lor(A\!\to\!\bot)$ pour mieux comparer à la logique intuitionniste où le $\lnot$ se comporte différemment.
Si on peut appliquer l'axiome $id$ à la formule $A$ (ce qui nécessite deux occurrences distinctes de la formule, une de chaque côté), c'est bien parce qu'on a conservé les deux parties de la formule initiale. Tandis qu'en logique intuitionniste, pour prouver $A\lor B$ c'est-à-dire $\;\vdash A\lor B$, les seules règles applicables sont $\lor R_{1}$ et $\lor R_{2}$ : il faut donc prouver $\;\vdash A$ ou prouver $\;\vdash B$ ; une fois qu'on a choisi lequel on va prouver, on n'a plus accès à l'autre. Ainsi, on ne peut pas prouver $A\lor(A\!\to\!\bot)$, car ni le séquent $\;\vdash A$ ni le séquent $\;\vdash (A\!\to\!\bot)$ n'est prouvable.

\vspace{.3em}

\def\widthhere{7cm}
\begin{floatingfigure}[r]{\widthhere}
\centering
\resizebox{\widthhere}{!}{
	\AXC	{}
	\RightLabel{(id)}
	\UIC	{$A \vdash A$}
	\RightLabel{(weak.\ R)}
	\UIC	{$A \vdash A, \bot$}
	\RightLabel{$(\to R)$}
	\UIC	{$\vdash A\,,\; A\!\to\!\bot$}
	\RightLabel{$(\lor R_{2})$}
	\UIC	{$\vdash A\,,\; A\lor(A\to\bot)$}
	\RightLabel{$(\lor R_{1})$}
	\UIC{$\vdash A\lor (A\to\bot)\,,\;A\lor (A\to\bot)$}
	\RightLabel{$(contr.\ R)$}
	\UIC{$\vdash A\lor(A\!\to\!\bot)$}
	\DP
%	\AXC	{}
%	\RightLabel{(id)}
%	\UIC	{$A \vdash A, \bot$}
%	\RightLabel{$(\to R)$}
%	\UIC	{$\vdash A\,,\; (A\to\bot)$}
%	\RightLabel{$(\to R)$}
%	\UIC	{$\vdash A\,,\; A\lor(A\to\bot)$}
%	\RightLabel{$(\lor R_{1})$}
%	\UIC{$\vdash A\lor (A\to\bot)\,,\;A\lor (A\to\bot)$}
%	\RightLabel{$(contr.\ R)$}
%	\UIC{$\vdash A\lor (A\to\bot)$}
%	\DP
}
%\caption{Preuve de $A\lor(A\!\to\!\bot)$ dans \LK}
%\label{fig:LKTiersExclu}
\end{floatingfigure}

\noindent
\textit{Remarque.} %Même avec la présentation de \LK\ de Gentzen où on a deux règles\LKouRun et\LKouRdeux au lieu de notre unique règle $\lor R$, pour montrer 
Même avec la présentation de \LK\ de Gentzen où on a deux règles $\lor R_{1}$ et $\lor R_{2}$ au lieu de notre unique règle $\lor R$, dans la preuve ci-contre de $A\lor(A\!\to\!\bot)$, on utilise une contraction à droite afin de conserver à la fois $A$ et $A\!\to\!\bot$. Le principe est le même que dans les équivalences d'écriture des règles en \ref{subsection:VariantesEcritureRegles}.

\ %vspace{.3em}

Finalement, on peut considérer que la logique intuitionniste est obtenue à partir de la logique classique en interdisant certaines règles structurelles. Une autre logique couramment étudiée, la logique linéaire, s'obtient à partir de la logique classique en conservant toutes les règles structurelles, mais en restreignant fortement leur utilisation.

\



%%%



\section{Logique linéaire (LL) et logique linéaire intuitionniste (ILL)}

La logique linéaire (LL) est une extension de la logique classique. Elle présente un grand un intérêt des points de vue logique aussi bien qu'informatique, notamment parce qu'elle contient une notion de ``ressources'' qui ne peuvent pas être utilisées inconsidérément. On peut considérer qu'elle est obtenue à partir de la logique classique en limitant l'usage des règles structurelles, si bien que de nouvelles nuances apparaissent, pour lesquelles sont introduits de nouveaux connecteurs. 

Voici un calcul de séquents pour la logique linéaire. Les séquents sont les mêmes que ceux de \LK\ : deux listes de formules $\G$ et $\D$, avec la notation $\G \vdash \D$. Les connecteurs, différents, sont donnés dans la figure~\ref{fig:connecteurs}. Les règles sont données dans la figure~\ref{fig:reglesLL}. Des explications sur ces connecteurs et ces règles sont proposées dans la sous-section~\ref{subsection:connecteursetc}, puis une interprétation en terme de ``ressources'' dans la sous-section~\ref{subsection:ressources}. Enfin, on présente la logique linéaire intuitionniste (ILL).%, obtenue à partir de la logique linéaire de la même manière que la logique intuitionniste est obtenue à partir de la logique classique.

%La logique linéaire intuitionniste (ILL) est obtenue à partir de la logique linéaire de la même manière que la logique intuitionniste est obtenue à partir de la logique classique : en se restreignant aux séquents avec une unique formule à droite ; ici, 
%
%Ses séquents sont ainsi les même que ceux de
%
%On construit la logique linéaire intuitionniste (notée ILL) en restreignant encore une fois les séquents de la logique linéaire (notée LL) à des séquents avec une unique formule à droite.


\begin{figure}[h]
\centering

\def\nomCol{
\resizebox{2.7cm}{!}{
\begin{tabular}{cc}
	\multicolumn{2}{c}{\'Elément neutre} \\
	Symbole & Nom
\end{tabular}
}
}

\begin{tabular}{|c|c|c|c|}
	\hline
	\multirow{2}{*}{\large{Symbole}} & \multirow{2}{*}{\large{Nom}} & \multicolumn{2}{c|}{\small{\'Elément neutre}} \\[-1mm] 
	& & \small{Symbole} & \small{Nom} \\ \hline
	$\otimes$ & \emph{tenseur /} & $\1$ & \emph{un} \\[-1.5mm]
	& \emph{produit tensoriel} & & \\ \hline
	$\&$ & \emph{avec} & $\top$ & \emph{top} \\ \hline
	$\oplus$ & \emph{plus} & $\0$ & \emph{zéro} \\ \hline
	$\revAnd$ & \emph{produit parallèle} & $\bot$ & \emph{bot} \\ \hline
	$\multimap$ & \emph{implication linéaire} & \multicolumn{2}{c|}{\scriptsize{pas d'élément neutre}} \\ \hline
	\multicolumn{4}{c}{Connecteurs binaires}
\end{tabular}
\qquad\qquad
\begin{tabular}{|c|c|}
	\hline
	\large{Symbole} & \large{Nom} \\ \hline
	$\lnot$ & \emph{non} \\ \hline
	$!$ & \emph{bang} \\ \hline
	$?$ & \emph{why not} \\ \hline
	\multicolumn{2}{c}{Connecteurs unaires}
\end{tabular}

\caption{Les connecteurs de la logique linéaire}
\label{fig:connecteurs}	
\end{figure}



%LL
%id, coupure : commun

%LL,règles logiques
\def\LLnonL{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(\lnot L)$}
	\UIC{$\G,\lnot A \vdash \D$}
	\DP
}
\def\LLnonR{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(\lnot R)$}
	\UIC{$\G \vdash \lnot A,\D$}
	\DP
}
\def\LLtenseurL{
	\AXC	{$\G,A,B \vdash \D$}
	\RightLabel{$(\otimes L)$}
	\UIC{$\G,A\otimes B \vdash \D$}
	\DP
}
\def\LLtenseurR{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G' \vdash B,\D'$}
	\RightLabel{$(\otimes R)$}
	\BIC{$\G \vdash A\otimes B,\D,\D'$}
	\DP
}
\def\LLavecLun{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(\& L_{1})$}
	\UIC{$\G,A\& B \vdash \D$}
	\DP
}
\def\LLavecLdeux{
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\& L_{2})$}
	\UIC{$\G,A\& B \vdash \D$}
	\DP
}
\def\LLavecR{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G \vdash B,\D$}
	\RightLabel{$(\& R)$}
	\BIC{$\G \vdash A\& B,\D$}
	\DP
}
\def\LLplusL{
	\AXC	{$\G,A \vdash \D$}
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\oplus L)$}
	\BIC{$\G,A\oplus B \vdash \D$}
	\DP
}
\def\LLplusRun{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(\oplus R_{1})$}
	\UIC{$\G \vdash A\oplus B,\D$}
	\DP
}
\def\LLplusRdeux{
	\AXC	{$\G \vdash B,\D$}
	\RightLabel{$(\oplus R_{2})$}
	\UIC{$\G \vdash A\oplus B,\D$}
	\DP
}
\def\LLparalleleL{
	\AXC	{$\G,A \vdash \D$}
	\AXC	{$\G',B \vdash \D'$}
	\RightLabel{$(\revAnd L)$}
	\BIC{$\G,\G',A\revAnd B \vdash \D,\D'$}
	\DP
}
\def\LLparalleleR{
	\AXC	{$\G \vdash A,B,\D$}
	\RightLabel{$(\revAnd R)$}
	\UIC{$\G \vdash A\revAnd B,\D$}
	\DP
}

%LL, constantes
\def\LLunL{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(\1 L)$}
	\UIC{$\G,\1 \vdash \D$}
	\DP
}
\def\LLunR{
	\AXC	{}
	\RightLabel{$(\1 R)$}
	\UIC{$\; \vdash \1$}
	\DP
}
\def\LLzeroL{
	\AXC	{}
	\RightLabel{$(\0 L)$}
	\UIC{$\0 \vdash \;$}
	\DP
}
\def\LLzeroR{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(\0 R)$}
	\UIC{$\G \vdash \0,\D$}
	\DP
}
\def\LLbotL{
	\AXC	{}
	\RightLabel{$(\bot L)$}
	\UIC{$\G,\bot \vdash \D$}
	\DP
}
\def\LLtopR{
	\AXC	{}
	\RightLabel{$(\top R)$}
	\UIC{$\G \vdash \top,\D$}
	\DP
}

%LL, modalités
\def\LLbangL{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(! L)$}
	\UIC{$\G,!A \vdash \D$}
	\DP
}
\def\LLbangR{
	\AXC	{$!\G \vdash A,?\D$}
	\RightLabel{$(! R)$}
	\UIC{$!\G \vdash !A,!\D$}
	\DP
}
\def\LLwhynotL{
	\AXC	{$!\G,A \vdash ?\D$}
	\RightLabel{$(? L)$}
	\UIC{$!\G,?A \vdash ?\D$}
	\DP
}
\def\LLwhynotR{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(? R)$}
	\UIC{$\G \vdash ?A,\D$}
	\DP
}

%LL, règles structurelles
\def\LLweakeningL{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(weakening\ L)$}
	\UIC{$\G,!A \vdash \D$}
	\DP
}
\def\LLweakeningR{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(weakening\ R)$}
	\UIC{$\G \vdash ?A,\D$}
	\DP
}
\def\LLcontractionL{
	\AXC	{$\G,!A,!A \vdash \D$}
	\RightLabel{$(contraction\ L)$}
	\UIC{$\G,!A \vdash \D$}
	\DP
}
\def\LLcontractionR{
	\AXC	{$\G \vdash ?A,?A,\D$}
	\RightLabel{$(contraction\ R)$}
	\UIC{$\G \vdash ?A,\D$}
	\DP
}
\def\LLexchangeL{
	\AXC	{$\G_{1},A,B,\G_{2} \vdash \D$}
	\RightLabel{$(exchange\ L)$}
	\UIC{$\G_{1},B,A,\G_{2} \vdash \D$}
	\DP
}
\def\LLexchangeR{
	\AXC	{$\G \vdash \D_{1},A,B,\D_{2}$}
	\RightLabel{$(exchange\ R)$}
	\UIC{$\G \vdash \D_{1},B,A,\D_{2}$}
	\DP
}





\begin{figure}[h]
\centering
{
\renewcommand{\arraystretch}{2}
\resizebox{\linewidth}{!}{
$
\begin{tabular}{|c|c|}
 	\hline
	\id & \cut \\ \hline
	\LLnonL & \LLnonR \\ \hline
	\LLtenseurL & \LLtenseurR \\ \hline
	\LLavecLun \; \LLavecLdeux & \LLavecR \\ \hline
	\LLplusL & \LLplusRun \; \LLplusRdeux \\ \hline
	\LLparalleleL & \LLparalleleR \\ \hline
%	\LLunL & \LLunR \\
%	\LLzeroL & \LLzeroR \\
%	\LLbotL & \LLtopR \\
%\hspace{-2.5mm}
	\LLunL \; \LLzeroL & \LLunR \; \LLzeroR \\ \hline
	\LLbotL & \LLtopR \\ \hline
%	\LLbangL & \LLbangR \\
%	\LLwhynotL & \LLwhynotR \\
	\LLbangL \; \LLwhynotL & \LLbangR  \; \LLwhynotR \\ \hline
	\LLweakeningL & \LLweakeningR \\ \hline
	\LLcontractionL & \LLcontractionR \\ \hline
	\LLexchangeL & \LLexchangeR \\ \hline
\end{tabular}
$
}
}
\caption{Les règles d'un calcul de séquents pour LL}
\label{fig:reglesLL}
\end{figure}





\subsection{Limitation des règles structurelles, nouveaux connecteurs, modalités}
\label{subsection:connecteursetc}

L'usage des règles structurelles est restreint à certains types de formules qu'on verra plus loin. Une conséquence importante est qu'on n'a plus l'équivalence entre les deux écritures possibles des règles liées à $\land$, vues en \ref{subsection:VariantesEcritureRegles}. C'est pourquoi on introduit deux connecteurs distincts $\otimes$ et $\&$, qui pourraient tous deux se lire ``et'' en première approximation, correspondant chacun à une définition possible de $\land$ dans \LK\ (cf. les règles liées à ces connecteurs dans la figure~\ref{fig:reglesLL}). De même, le $\lor$ de logique classique est séparé en deux connecteurs $\oplus$ et $\revAnd$. On souhaite tout de même garder des règles structurelles, dont l'usage est seulement limité. On introduit pour cela des connecteurs unaires \emph{modaux}, ou \emph{modalités}, $!$ et $?$. Les règles structurelles peuvent seulement être appliquées à des formules comportant un de ces connecteurs : $!$ si la formule est à gauche, $?$ si elle est à droite. On a aussi des règles d'introduction de ces connecteurs, où par exemple $!\G$ représente une liste dont toutes les formules sont de la forme $!A$. On retrouve des relations entre $\otimes$ et $\&$ grâce à ces modalités : par exemple $(!A)\otimes(!B) = !(A\& B)$, qu'on discute dans la prochaine sous-section.



\subsection{Interprétation : une idée de ``ressources''}
\label{subsection:ressources}

La logique linéaire est une logique de ``ressources''. L'implication linéaire $A\multimap B$ peut en effet se comprendre comme ``on peut dépenser un objet de type $A$ pour obtenir un objet de type $B$''. Ici, ``type'' est simplement un mot du langage courant et non un terme mathématique ou informatique.% ; on aurait aussi bien pu dire ``un objet de sorte $A$'', mais cela  
La formule $A\otimes B$ représente la possession à la fois d'un objet de type $A$ et d'un autre objet de type $B$. On comprend alors la notion de ``dépense'' en remarquant que, si on a $A\multimap B$ et $A\multimap C$, on n'obtient pas pour autant $A\multimap B\otimes C$ car un seul objet de type $A$ ne peut pas être dépensé deux fois ; en revanche on obtient bien $A\otimes A \multimap B\otimes C$. On n'obtient pas non plus $A\otimes A\otimes A \multimap B\otimes C$ car un objet de type $A$ ne serait pas dépensé ; cela est lié au fait que la formule $A\multimap\1$ n'est pas universellement valide, où $\1$ est l'élément neutre pour $\otimes$. Une analogie courante et pertinente compare ce fragment de la logique linéaire aux équations de réaction en chimie, avec la maxime de Lavoisier bien connue ``Rien ne se perd, rien ne se crée, tout se transforme''. Par exemple, l'équation $CH_{4} + 2O_{2} \longrightarrow CO_{2} + 2H_{2}O$ peut être fidèlement représentée par la formule $!(CH_{4}\otimes O_{2}\otimes O_{2} \multimap CO_{2}\otimes H_{2}O\otimes H_{2}O)$, où $CH_{4}$ etc. sont considérés comme des constantes ; la modalité $!$, discutée plus loin, signifie qu'on peut appliquer cette réaction autant de fois qu'on le souhaite.

Intéressons-nous maintenant à $\&$, l'autre connecteur issu du $\land$ du logique classique. La formule $A\& B$ représente la possession, au choix, d'un objet de type $A$ ou d'un objet de type $B$. \`A partir de $A\multimap B$ et $A\multimap C$, on obtient bien $A\multimap B\&C$ : on a le choix de la fa\c con dont on dépense l'objet de type $A$. Le fait qu'on ne possède finalement qu'un seul objet, de type $A$ ou de type $B$, peut donner l'impression qu'il s'agit d'une disjonction. Le point important est la possibilité de choisir soi-même le type parmi $A$ et $B$. Cela entraîne qu'on peut prouver $A\&B\multimap A$ et $A\&B\multimap B$, mais pas $A\multimap A\&B$, donc il s'agit bien d'une conjonction. C'est $A\oplus B$ qui représente la possession d'un objet dont le type est $A$ ou $B$, sans possibilité de choisir. Il s'agit cette fois d'une disjonction : on peut prouver $A\multimap A\oplus B$ mais pas $A\oplus B\multimap A$.

La modalité $!A$ signifie qu'on peut posséder un objet de type $A$ autant de fois qu'on le souhaite, y compris zéro. Un exemple significatif est l'égalité $(!A)\otimes(!B) = !(A\& B)$ : posséder simultanément autant d'objets qu'on veut de type $A$ et autant d'objets qu'on veut de type $B$, revient à avoir autant de fois qu'on veut la possibilité de choisir de posséder un objet de type $A$ ou un objet de type $B$.


\subsection{Logique linéaire intuitionniste}

La logique linéaire intuitionniste (ILL) peut être associée à un calcul dont les séquents sont les mêmes que ceux de \LJ\ : une liste de formules $\G$ et une formule $D$, avec la notation $\G \vdash D$ ; les règles sont données dans la figure~\ref{fig:reglesILL}.

La logique linéaire intuitionniste (ILL) est obtenue à partir de la logique linéaire de la même manière que la logique intuitionniste est obtenue à partir de la logique classique : en se restreignant aux séquents avec une unique formule à droite. 
On ne garde que les connecteurs binaires $\otimes$, $\multimap$ et $\&$ avec leur élément neutre éventuel, et le connecteur modal $!$.
Le $\lnot$ disparaît pour la même raison qu'en logique intuitionniste. (?) Les autres connecteurs sont liés à des règles qui ne s'adaptent pas aux séquents avec une unique formule à droite, à l'exception de $\oplus$ qui pourrait être conservé sans problème. Néanmoins, on se conforme à une tradition qui consiste à parler de \emph{logique linéaire intuitionniste} lorsqu'on ne considère pas de connecteur $\oplus$, et de \emph{logique linéaire intuitionniste à produits finis} lorsqu'on le rajoute.
%Le connecteur de logique linéaire $\revAnd$ disparaît alors, puisque les règles liées nécessitent des séquents avec plusieurs formules à droite. Le connecteur $\oplus$ peut être conservé, mais traditionnellement, il est absent de ce qu'on appelle la \emph{logique linéaire intuitionniste}, qui devient la \emph{logique linéaire intuitionniste à produits finis} lorsqu'on le rajoute.

\begin{figure}[h]
\centering
{
%\renewcommand{\arraystretch}{2}
%\resizebox{\linewidth}{!}{
%}
\idees{à faire}
}
\caption{Les règles d'un calcul de séquents pour ILL}
\label{fig:reglesILL}
\end{figure}



\section{Le procédé d'élimination de la coupure (p.é.c.) pour ILL}

coupure intéressante : transitivité, conclusion prouvée à son tour utilisée comme hypothèse

souvent, coupure non nécessaire : correct et surtout complet sans.
mais démonstration souvent difficile

démonstration également intéressante. constructive : transformer n'importe quelle preuve en preuve du même séquent sans coupure. pour cela, énumération de transformations élémentaires : agissent sur petite partie de la preuve.

ces transformations permettent de définir une relation d'équivalence



\section{}

\idees{
On suppose qu'on a présenté la logique linéaire et son calcul de séquents, et le procédé d'élimination de la coupure (qu'on abrège pour l'instant parfois en p.é.c., mais cela peut changer). On distingue le p.é.c. ``strict'' avec seulement les transformations nécessaires pour le théorème d'élimination de la coupure, du p.é.c. ``large'' où on a rajouté les transformations qui font que alpha, lambda, rho deviennent des isomorphismes et donc permettent d'obtenir vraiment une catégorie monoïdale.
}


\subsection{Invariant modulaire et catégorie}

\noindent
\textbf{Notation.}
Soit $p$ une preuve du séquent $\s$. On écrit%
	\AXC	{$p$}
	%\noLine
	\dottedLine
	\UIC{$\;\s\;$}
	\DP
car souvent, il est intéressant d'expliciter $\s$ pour prolonger $p$ en une preuve plus complexe d'un autre séquent.

\

\`A chaque preuve $p$, on associe une \textbf{\emph{dénotation}} $[p]$. On veut que cela constitue un invariant selon l'élimination de la coupure : deux preuves on la même dénotation si, et seulement si, une peut être obtenue en appliquant à l'autre des transformations autorisée par la procédure d'élimination de la coupure. Ceci est motivé par une analogie avec la théorie des n\oe uds, où les invariants sont relatifs aux transformations de Reidemeister.
\idees{expliquer ? ne pas en parler ?}

\begin{floatingfigure}[r]{0.27\textwidth}
\centering
	\AXC	{$p_{1}$}
	\dottedLine
	\UIC{$A\vdash B$}
	\AXC	{$p_{2}$}
	\dottedLine
	\UIC{$B\vdash C$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash C$}
	\noLine
	\UIC{}
	\noLine
	\UIC{$p$}
	\DP\\
\end{floatingfigure}


On demande également la propriété suivante. Soit $A$, $B$, $C$ des formules, et $p_{1}$ et $p_{2}$ des preuves de $A \vdash B$ et $B \vdash C$ respectivement. La règle de coupure fournit immédiatement la preuve $p$ de $A \vdash C$ ci-contre. On veut que sa dénotation $[p]$ se déduise à partir de $[p_{1}]$ et $[p_{2}]$. On introduit pour cela la \textbf{loi de composition $\circ$} telle que $[p] = [p_{2}] \circ [p_{1}]$.
On dit alors que l'invariant est \emph{modulaire}.

On remarque que la loi de composition $\circ$ est \textbf{associative} et présente pour chaque formule une \textbf{identité} à gauche et à droite. En effet, soit $A$, $B$, $C$, $D$ des formules, et $p_{1}$, $p_{2}$, $p_{3}$ des preuves respectives de $A \vdash B$, $B \vdash C$, $C \vdash D$. On peut en déduire deux preuves de $A \vdash D$ :
\\
\begin{center}
	\AXC	{$p_{1}$}
	\dottedLine
	\UIC{$A\vdash B$}
	\AXC	{$p_{2}$}
	\dottedLine
	\UIC{$B\vdash C$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash C$}
	\AXC	{$p_{3}$}
	\dottedLine
	\UIC{$C\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash D$}
	\DP
\quad et \quad
	\AXC	{$p_{1}$}
	\dottedLine
	\UIC{$A\vdash B$}
	\AXC	{$p_{2}$}
	\dottedLine
	\UIC{$B\vdash C$}
	\AXC	{$p_{3}$}
	\dottedLine
	\UIC{$C\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$B\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash D$}
	\DP
\end{center}

\noindent
qui sont équivalentes d'après le procédé d'élimination de la coupure. Cela signifie précisément que $([p_{1}]\circ[p_{2}])\circ[p_{3}] = [p_{1}]\circ([p_{2}]\circ[p_{3}])$. L'identité pour une formule $A$ est la dénotation de la preuve 
{\centerAlignProof
	\AXC	{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	\DP
}
; on note cette dénotation $id_{A}$. Soit $p$ une preuve de $A \vdash B$, les deux preuves 
%\\
%%\def\proofSkipAmount{\vskip -10em}
%\begin{center}
	\AXC	{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	\AXC	{$p$}
	\dottedLine
	\UIC{$A\vdash B$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash B$}
	\DP
%\qquad\qquad et \qquad\qquad
et
	\AXC	{$p$}
	\dottedLine
	\UIC{$A\vdash B$}
	\DP
%\end{center}
%
%\noindent
sont équivalentes selon l'élimination de la coupure, ce qui signifie que $[p]\circ id_{A}=[p]$. De même, pour $p$ une preuve de $B\vdash A$, on a $id_{A}\circ[p]=[p]$.

Ces propriétés sur les dénotations permettent d'utiliser le formalisme des catégories.

% def catégorie
\begin{df}
Une \textbf{\emph{catégorie}} consiste en une collection d'\emph{objets} et une collection de \emph{morphismes}, cette dernière munie d'une opérations binaire partielle $\circ$ appelée \emph{composition}, avec les propriétés suivantes.

\begin{itemize}

\item
\`A chaque morphisme $f$ est associé un couple d'objets $(A,B)$ ; on écrit $f:A\to B$.
On dit que $A\to B$ est le \emph{type} de $f$, et que $f$ est un morphisme \emph{de} $A$ \emph{vers} $B$, et encore que $A$ est le \emph{domaine} ou la \emph{source} de $f$, et $B$ le \emph{codomaine} ou la \emph{cible} de $f$.

\item
Pour tous objets $A$, $B$, $C$ et morphismes $f$, $g$ tels que $f:A\to B$ et $g:B\to C$, le morphisme $g\circ f$ existe et $g\circ f:A\to C$.

\item
\textbf{Identité.}
Pour tout objet $A$, il existe un morphisme particulier $id_{A}:A\to A$ appelé \emph{identitié sur $A$}, tel que 
%pour tous objet $B$ et morphismes $f:B\to A$ et $g:A\to B$, $id_{A}\circ f=f$ et $g\circ id_{A}=g$.
pour tout objet $B$, pour tout $f:B\to A$, $id_{A}\circ f=f$ et pour tout $g:A\to B$, $g\circ id_{A}=g$.

\item
\textbf{Associativité.}
Pour tous $f:A\to B$, $g:B\to C$, $h:C\to D$, on a $h\circ(g\circ f) = (h\circ g)\circ f$.
\end{itemize}

\

\noindent\textbf{Notations.} Soit $\Ccal$ une catégorie, on note $Obj(\Ccal)$ la classe de ses objets et $Hom(\Ccal)$ celle de ses morphismes. Pour des objets $A$ et $B$, on note $Hom_{A\to B}(\Ccal)$ la classe des morphismes de type $A\to B$. On peut omettre l'argument $\Ccal$ s'il n'y a pas d'ambiguïté, par exemple si on travaille sur une seule catégorie.
\end{df}

Les dénotations des preuves s'organisent sous la forme de la catégorie suivante, qu'on appellera $\CProofs$.
\idees{choix du nom ?}
\`A chaque formule $A$ on associe une dénotation $[A]$. Les dénotations des formules, deux à deux distinctes, constituent les objets de la catégorie. Les morphismes sont des dénotations de preuves. Les morphismes de $[A]$ vers $[B]$ sont les dénotations des preuves de $A\vdash B$ ; si ce séquent n'est pas prouvable, il n'y en a pas. Comme on l'a vu, la composition $\circ$ est bien associative, et l'identité sur $[A]$ est le morphisme $id_{A}$. On notera aussi $A$ la dénotation de la formule $A$, sauf lorsqu'on souhaite insister sur le fait qu'il s'agit d'une dénotation. Cela permet d'alléger l'écriture et de retrouver la notation employée dans les définitions sur les catégories.

%Pour l'instant, on ne considère 
Cette définition ne tient compte que des dénotations de preuves où le séquent prouvé a une unique formule à gauche et une unique formule à droite ; les autres séquents des preuves peuvent avoir n'importe quelle forme. Cette restriction est conservée dans 
\idees{quelques sous-sections ? tout le reste de la partie ? toujours ? Est-ce qu'on peut étendre ce formalisme à n'importe quelle preuve de la logique intuitionniste linéaire grâce au théorème de MacLane, cf. mail ?}
%les quelques sous-section qui suivent, où on enrichit notre catégorie. ... Enfin, on étend ce formalisme à toutes les preuves de la logique linéaire intuitionniste, %c'est-à-dire avec des séquents qui 
%où les séquents ont un nombre quelconque de formules à gauche et exactement une formule à droite.

%


\subsection{Produit tensoriel et bifoncteur}

\def\sizedefotimes{0.27\textwidth}
\begin{floatingfigure}[r]{\sizedefotimes}
\centering
\resizebox{\sizedefotimes}{!}{
	\AXC	{$p_{1}$}
	\dottedLine
	\UIC{$A_{1}\vdash B_{1}$}
	   \AXC{$p_{2}$}
	   \dottedLine
	   \UIC{$A_{2}\vdash B_{2}$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A_{1},A_{2} \vdash B_{1}\otimes B_{2}$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A_{1}\otimes A_{2} \vdash B_{1}\otimes B_{2}$}
	\noLine
	\UIC{}
	\noLine
	\UIC{$p$}
\DP
}
\\
\end{floatingfigure}




%Le connecteur de logique linéaire $\otimes$ permet de définir un produit tensoriel sur les dénotations de formules, aussi noté $\otimes$ : on pose $[A]\otimes[B]=[A\otimes B]$.
Le connecteur de logique linéaire $\otimes$, appelé \emph{produit tensoriel}, induit un opérateur sur les dénotations de formules, aussi noté $\otimes$ : on pose $[A]\otimes[B]=[A\otimes B]$.
On souhaite étendre cet opérateur aux dénotations de preuves. Pour cela, on remarque qu'à partir de preuves $p_{1}$ de $A_{1}\vdash B_{1}$ et $p_{2}$ de $A_{2}\vdash B_{2}$
%	\AXC	{$p_{1}$}
%	\dottedLine
%	\UIC{$A_{1}\vdash B_{1}$}
%	\DP
%et
%	\AXC	{$p_{2}$}
%	\dottedLine
%	\UIC{$A_{2}\vdash B_{2}$}
%	\DP
, on peut déduire la preuve $p$ ci-contre de $A_{1}\otimes A_{2} \vdash B_{1}\otimes B_{2}$. On définit alors ${[p_{1}]\otimes[p_{2}]=[p]}$.

L'opérateur est bien défini : si on a deux autres preuves $p'_{1}$ et $p'_{2}$ telles que $[p'_{1}]=[p_{1}]$ et $[p'_{2}]=[p_{2}]$, et si $p'$ est la preuve obtenue à partir de $p'_{1}$ et $p'_{2}$ selon le procédé utilisé pour construire $p$, alors $[p']=[p]$. En effet, le procédé d'élimination de la coupure autorise évidemment à remplacer la preuve $p_{1}$ apparaissant dans la preuve $p$ par une preuve $p'_{1}$ qui lui est équivalente.

Intéressons-nous à la compatibilité de $\otimes$ avec $\circ$. Soit des morphismes de  $f_{1} : A_{1}\to B_{1}$\,,\; $f_{2} : A_{2}\to B_{2}$\,,\; $g_{1} : B_{1}\to C_{1}$\,,\; $g_{2} : B_{2}\to C_{2}$ %(on confond un objet avec sa dénotation pour simplifier l'écriture)
. Pour chaque morphisme $f$, soit $p(f)$ une preuve de dénotation $f$. Les deux preuves suivantes sont équivalentes selon le p.é.c., ce qui signifie que $(g_{1}\otimes g_{2})\circ(f_{1}\otimes f_{2}) = (g_{1}\circ f_{1})\otimes(g_{2}\circ f_{2})$.
\vspace{2mm}

\noindent
\resizebox{\linewidth}{!}{
	\AXC	{$p(f_{1})$}
	\dottedLine
	\UIC{$A_{1}\vdash B_{1}$}
		\AXC{$p(f_{2})$}
		\dottedLine
		\UIC{$A_{2}\vdash B_{2}$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A_{1},A_{2} \vdash B_{1}\otimes B_{2}$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A_{1}\otimes A_{2} \vdash B_{1}\otimes B_{2}$}
		\AXC	{$p(g_{1})$}
		\dottedLine
		\UIC{$B_{1}\vdash C_{1}$}
			\AXC{$p(g_{2})$}
			\dottedLine
			\UIC{$B_{2}\vdash C_{2}$}
		\RightLabel{$(\otimes R)$}
		\BIC{$B_{1},B_{2} \vdash C_{1}\otimes C_{2}$}
		\RightLabel{$(\otimes L)$}
		\UIC{$B_{1}\otimes B_{2} \vdash C_{1}\otimes C_{2}$}
	\RightLabel{$(cut)$}
	\BIC{$A_{1}\otimes A_{2} \vdash C_{1}\otimes C_{2}$}
\DP
\quad
	\AXC	{$p(f_{1})$}
	\dottedLine
	\UIC{$A_{1}\vdash B_{1}$}
		\AXC	{$p(g_{1})$}
		\dottedLine
		\UIC{$B_{1}\vdash C_{1}$}
	\RightLabel{$(cut)$}
	\BIC{$A_{1} \vdash C_{1}$}
		\AXC{$p(f_{2})$}
		\dottedLine
		\UIC{$A_{2}\vdash B_{2}$}
			\AXC{$p(g_{2})$}
			\dottedLine
			\UIC{$B_{2}\vdash C_{2}$}
		\RightLabel{$(cut)$}
		\BIC{$A_{2} \vdash C_{2}$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A_{1},A_{2} \vdash C_{1}\otimes C_{2}$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A_{1}\otimes A_{2} \vdash C_{1}\otimes C_{2}$}		
\DP
}
\vspace{1mm}

\noindent
On a également l'égalité $id_{[A]\otimes[B]}=id_{[A]}\otimes id_{[B]}$ en raison de l'équivalence des preuves
\vspace{1.5mm}

\noindent
\resizebox{3cm}{!}{
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A\otimes B \vdash A\otimes B$}
	\DP
}
et
\resizebox{4cm}{!}{
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A \vdash A$}
		\AXC{}
		\RightLabel{$(id)$}
		\UIC{$B \vdash B$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A,B \vdash A\otimes B$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes B \vdash A\otimes B$}
	\DP
}
.
\vspace{3mm}

\


Ces propriétés signifient que l'opérateur $\otimes$ constitue un \textbf{bifoncteur} sur $\CProofs$, c'est-à-dire un foncteur de $\CProofs\times\CProofs$ vers $\CProofs$, avec les définitions suivantes.

\

\begin{df}
Soit $\Ccal$ et $\Dcal$ des catégories. Un \textbf{\emph{foncteur}} $F:\Ccal\longrightarrow\Dcal$ de $\Ccal$ vers $\Dcal$ consiste en une application des objets de $\Ccal$ vers les objets de $\Dcal$ et une application des morphismes de $\Ccal$ vers les morphismes de $\Dcal$, notées toutes deux $F$ par abus d'écriture, tel que
\begin{itemize}
\item
Pour tous objets $A$, $B$ et morphisme $f:A\to B$ de $\Ccal$, on a $F(f):F(A)\to F(B)$.
\item
Pour tous morphismes $f:A\to B$ et $g:B\to C$ de $\Ccal$, on a $F(g\circ f)=F(g)\circ F(f)$.
\item
Pour tout objet $A$ de $\Ccal$, on a $F(id_{A})=id_{F(A)}$.
\end{itemize}
\end{df}

\begin{df}
Soit $\Ccal$ et $\Dcal$ des catégories. On définit la \textbf{\emph{catégorie produit}} $\Ccal\times\Dcal$, où ${Obj(\Ccal\times\Dcal)}=Obj(\Ccal)\times Obj(\Dcal)$ et ${Hom_{(A,A')\to(B,B')}=Hom_{A\to B}(\Ccal)\times Hom_{A' \to B'}(\Dcal)}$.
\end{df}


%


\subsection{Catégorie monoïdale}

%On verra que munir la catégorie $\CProofs$ de $\otimes$ en fait une catégorie monoïdale. Mais pour définir ce qu'est une catégorie monoïdale, il faut plusieurs autres définitions sur les catégories.

La catégorie $\CProofs$ ayant été munie du bifoncteur $\otimes$, on se demande s'il s'agit d'une catégorie monoïdale, dont la définition est donnée après deux définitions auxiliaires.
%, qui s'appuie sur les deux définitions suivantes, est donnée ensuite.
%à leur suite.

\begin{df}
Soit $\Ccal$ et $\Dcal$ des catégories. Soit $F,G:\Ccal\longrightarrow\Dcal$ des foncteurs de $\Ccal$ vers $\Dcal$. Une \textbf{\emph{transformation naturelle}} $\theta$ de $F$ vers $G$ est une famille $\big(\;\theta_{A} : F(A) \to G(A)\;\big)_{A\in Obj(\Ccal)}$ de morphismes de $\Dcal$, indexée par les objets de $\Ccal$, telle que le diagramme suivant commute dans $\Dcal$ pour tout morphisme $f:A\to B$ de $\Ccal$

{\centering
%\includegraphics[width=0.4\linewidth]{diagrammeNaturel}
\begin{tikzcd}
	F(A) \arrow[r,"\theta_{A}"] \arrow[d,"F(f)"] & G(A) \arrow[d,"G(f)"] \\
	F(B) \arrow[r,"\theta_{B}"] & G(B)
\end{tikzcd}

}
\noindent
On note $\theta : F \Rightarrow G$, voire $\theta : F \Rightarrow G : \Ccal \longrightarrow \Dcal$.
\end{df}




\begin{df}
Soit une catégorie,
un morphisme $f:A\to B$ est un \textbf{\emph{isomorphisme}} s'il existe un morphisme $f^{-1}:B\to A$ tel que $f^{-1}\circ f=id_{A}$ et $f\circ f^{-1}=id_{B}$.
\end{df}


% def catégorie monoïdale
\begin{df}
Une \textbf{\emph{catégorie monoïdale}} est une catégorie $\Ccal$ munie d'un bifoncteur \\${\otimes:\Ccal\times\Ccal \longrightarrow \Ccal}$ (pour lequel on utilise une notation infixe) avec d'un objet particulier $e$, %tel que les morphismes suivants existent
%
%\begin{itemize}
%\item
%pour tous objets $A$, $B$, $C$, un isomorphisme naturel
%%\linebreak
%${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$ permettant de parler d'associativité ;
%\item
%pour tout objet $A$, des isomorphismes naturels ${\lambda_{A}:e\otimes A\to A}$ et ${\rho_{A}:A\otimes e\to A}$ %justifiant le nom d'\textbf{unité} pour $e$ ;
%permettant d'appeler $e$ l'objet \textbf{unité} ;
%\end{itemize}
telle qu'il existe des \textbf{isomorphismes naturels}
\begin{itemize}
\item
${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$, permettant de parler d'associativité ;
\item
${\lambda_{A}:e\otimes A\to A}$ ;
\item
${\rho_{A}:A\otimes e\to A}$,
permettant avec le précédent d'appeler $e$ l'objet \textbf{unité} ;
\end{itemize}

\noindent
et tel que les diagrammes suivants commutent pour tous objets $A$, $B$, $C$, $D$. 

%\includegraphics[width=\linewidth]{diagrammeMonoidalePenta}
%{\centering
%\includegraphics[width=0.5\linewidth]{diagrammeMonoidaleTri}
%			
%}
{\centering
\begin{tikzcd}
	& (A\otimes B)\otimes(C\otimes D) \arrow[rd,"\alpha_{A,B,C\otimes D}"] & \\
	((A\otimes B)\otimes C)\otimes D 
			\arrow[ru,"\alpha_{A\otimes B,C,D}"] 
			\arrow[d,"\alpha_{A,B,C}\otimes id_{D}"] 
		& & A\otimes(B\otimes(C\otimes D)) \\
	(A\otimes(B\otimes C)\otimes D 
			\arrow[rr,"\alpha_{A,B\otimes C,D}"]
		& & A\otimes((B\otimes C)\otimes D)
			\arrow[u,"id_{A}\otimes\alpha_{B,C,D}"]
\end{tikzcd}
\begin{tikzcd}
	(A\otimes e)\otimes B
			\arrow[rr,"\alpha_{A,e,B}"]
			\arrow[rd,"\rho_{A}\otimes id_{B}"]
		& & A\otimes(e\otimes B)
			\arrow[ld,"id_{A}\otimes\lambda_{B}"] \\
	& A\otimes B &
\end{tikzcd}
			
}
\noindent
Ces diagrammes sont appelés \emph{diagrammes pentagonal} et \emph{triangulaire}. Leur commutativité est la \emph{propriété de cohérence}.
%On a omis les indices de $\alpha$, $\lambda$, $\rho$ et écrit $A$ pour $id_{A}$ : par exemple, comprendre $\alpha\otimes D$ comme $\alpha_{A,B,C}\otimes id_{A}$.
%On omet souvent les indices de $\alpha$, $\lambda$, $\rho$ lorsqu'il n'y a pas d'ambiguïté. Ici, on écrit aussi $A$ pour $id_{A}$ : par exemple, comprendre $\alpha\otimes D$ comme ${\alpha_{A,B,C}\otimes id_{A}}$.

\vspace{2mm}

\noindent\textbf{Précision.} On parle d'isomorphismes naturels car il y a bien des transformations naturelles associées. En fait la condition sur $\alpha$ est $\alpha : F \Rightarrow G :\Ccal\times\Ccal\times\Ccal \longrightarrow \Ccal$ avec $F:(x,y,z) \mapsto (x\otimes y)\otimes z$ et $G:(x,y,z) \mapsto x\otimes(y\otimes z)$ où $x$, $y$, $z$ sont trois objets ou trois morphismes de $\Ccal$. On écrit $\alpha_{A,B,C}$ pour $\alpha_{(A,B,C)}$. Les $\alpha_{A,B,C}$ sont bien des morphismes de $\Ccal$, qui est la catégorie d'arrivée de $F$ et $G$. On veut aussi ${\lambda : (e\otimes\bullet) \Rightarrow Id_{\Ccal} : \Ccal \longrightarrow \Ccal}$ où $(e\otimes\bullet) : \left\{
\begin{array}{l}
	A \mapsto e\otimes A \\
	f \mapsto id_{e}\otimes f
\end{array}
\right.$ et où $Id_{\Ccal}$ est le foncteur identité sur $\Ccal$. Il y a une condition similaire pour $\rho$.


\end{df}


%L'opérateur $\otimes$ qu'on a définit sur les dénotations de formules puis sur les dénotations de preuves constitue un \emph{bifoncteur} de la catégorie $\CProofs$, c'est-à-dire un foncteur $\otimes : \Ccal\times\Ccal \longrightarrow \Ccal$. $\Ccal$ munie de $\otimes$ est presque une \textbf{catégorie monoïdale} avec comme objet unité $[\1]$, la dénotation de la formule $\1$, élément neutre pour le connecteur $\otimes$.

On munit la catégorie $\CProofs$ du bifoncteur $\otimes$ et on choisit comme objet unité $[\1]$, la dénotation de la formule $\1$, élément neutre pour le connecteur $\otimes$. On obtient une \textbf{catégorie monoïdale}, ou presque une catégorie monoïdale, selon les choix effectués pour définir le procédé d'élimination de la coupure.
%La catégorie $\Ccal$ munie du bifoncteur $\otimes$ est presque une \textbf{catégorie monoïdale} avec comme objet unité $[\1]$, la dénotation de la formule $\1$, élément neutre pour le connecteur $\otimes$.
En effet, on peut définir des morphismes $\alpha_{A,B,C}$, $\lambda_{A}$, $\rho_{A}$ comme les dénotations des preuves suivantes\\

\def\alphaABC{
$\alpha_{A,B,C}$ :
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	  \AXC{}
	  \RightLabel{$(id)$}
	  \UIC{$B\vdash B$}
	    \AXC{}
	    \RightLabel{$(id)$}
	    \UIC{$C\vdash C$}
	  \RightLabel{$(\otimes R)$}
	  \BIC{$B,C \vdash B\otimes C$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A,B,C \vdash A\otimes(B\otimes C)$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes B,C \vdash A\otimes(B\otimes C)$}
	\RightLabel{$(\otimes L)$}
	\UIC{$(A\otimes B)\otimes C \vdash A\otimes(B\otimes C)$}
\DP
}

\def\lambdaA{
$\lambda_{A}$ :
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	\RightLabel{$(\1 L)$}
	\UIC{$\1,A\vdash A$}
	\RightLabel{$(\otimes L)$}
	\UIC{$\1\otimes A\vdash A$}
\DP
}

\def\rhoA{
$\rho_{A}$ : 
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	\RightLabel{$(\1 L)$}
	\UIC{$A,\1\vdash A$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes\1\vdash A$}
\DP
}

%\begin{tabular}{p{0.5\linewidth}p{0.5\linewidth}}
%	\multicolumn{2}{l}{\quad\; \alphaABC} \\ \\
%	\lambdaA & \rhoA
%\end{tabular}
% alpha : \qquad\qquad, 2 autres : \quad

\noindent
\resizebox{\linewidth}{!}{
\alphaABC \quad \lambdaA \quad \rhoA
}

\

\noindent
Il s'agit bien de transformations naturelles, et on a bien la propriété de cohérence.
\idees{démonstrations, ou au moins explications, exemples de preuves équivalentes ?}
%
La définition d'une \emph{catégorie monoïdale} exige que ces morphismes $\alpha$, $\lambda$, $\rho$ soient des isomorphismes. On n'écrit plus les indices, qui sont toujours les mêmes. On peut définir des morphismes naturels $\bar\alpha_{A,B,C} : A\circ(B\circ C) \to (A\circ B)\circ C$ \,,\; $\bar\lambda_{A} : A \to e\circ A$ \,,\; $\bar\rho_{A} : A \to A\circ e$\,, par exemple $\bar\lambda_{A}$ est la dénotation de 
\resizebox{3cm}{!}{
	\AXC{}
	\RightLabel{$(\1 R)$}
	\UIC{$\vdash \1$}
		\AXC{}
		\RightLabel{$(id)$}
		\UIC{$A\vdash A$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A\vdash \1\otimes A$}
\DP
}
. On pense naturellement à ces morphismes quand on cherche des inverses de $\alpha$, $\lambda$ et $\rho$. On a bien ${\lambda\circ\bar\lambda = id_{A}}$ et ${\rho\circ\bar\rho=id_{A}}$. En revanche, si on considère un p.é.c. ``strict'', on n'a aucune des égalités suivantes : ${\bar\lambda\circ\lambda = id_{e\circ A}}$\,,\; ${\bar\rho\circ\rho=id_{A\circ e}}$\,,\; ${\bar\alpha\circ\alpha=id_{(A\circ B)\circ C}}$\,,\; ${\alpha\circ\bar\alpha=id_{A\circ(B\circ C)}}$\,. Si on considère au contraire la version plus ``large'' du p.é.c., on a bien toutes ces égalités. Les transformations autorisées qui ont été ajoutées au p.é.c. strict pour obtenir cette version large sont d'ailleurs expressément choisies pour satisfaire ces égalités. Dans la suite, on suppose toujours qu'on a choisi la version ``large'' du p.é.c., donc $\CProofs$ est bien une catégorie monoïdale.
%On obtient alors que $\CProofs$ est une \textbf{catégorie monoïdale}. Dans la suite, on se met toujours dans ce cas



%En revanche, si on considère un p.é.c. ``strict'', ces morphismes $\alpha$, $\lambda$, $\rho$ ne sont pas des isomorphismes. On n'écrit plus les indices, qui sont toujours les mêmes. On peut définir des morphismes naturels $\bar\alpha_{A,B,C} : A\circ(B\circ C) \to (A\circ B)\circ C$ \,,\; $\bar\lambda_{A} : A \to e\circ A$ \,,\; $\bar\rho_{A} : A \to A\circ e$\,, par exemple $\bar\lambda_{A}$ est la dénotation de 
%\resizebox{3cm}{!}{
%	\AXC{}
%	\RightLabel{$(\1 R)$}
%	\UIC{$\vdash \1$}
%		\AXC{}
%		\RightLabel{$(id)$}
%		\UIC{$A\vdash A$}
%	\RightLabel{$(\otimes R)$}
%	\BIC{$A\vdash \1\otimes A$}
%\DP
%}
%. On pense naturellement à ces morphismes quand on cherche des inverses de $\alpha$, $\lambda$ et $\rho$. On a bien ${\lambda\circ\bar\lambda = id_{A}}$ et ${\rho\circ\bar\rho=id_{A}}$, mais on n'a aucune des égalités suivantes : ${\bar\lambda\circ\lambda = id_{e\circ A}}$\,,\; ${\bar\rho\circ\rho=id_{A\circ e}}$\,,\; ${\bar\alpha\circ\alpha=id_{(A\circ B)\circ C}}$\,,\; ${\alpha\circ\bar\alpha=id_{A\circ(B\circ C)}}$\,.
%
%Cependant en ajoutant des transformations ...



%\def\noninverseUn{
%	\AXC{}
%	\RightLabel{$(id)$}
%	\UIC{$A\vdash A$}
%	\RightLabel{$(\1 L)$}
%	\UIC{$A,\1\vdash A$}
%	\RightLabel{$(\otimes L)$}
%	\UIC{$A\otimes\1\vdash A$}
%		\AXC{}
%		\RightLabel{$(id)$}
%		\UIC{$A\vdash A$}
%			\AXC{}
%			\RightLabel{$(\1 R)$}
%			\UIC{$\vdash \1$}
%		\RightLabel{$(\otimes R)$}
%		\BIC{$A\vdash A\otimes\1$}
%	\RightLabel{$(cut)$}
%	\BIC{$A\otimes\1\vdash A\otimes\1$}
%	\noLine
%	\UIC{}
%	\noLine
%	\UIC{$p_{1}$}
%\DP
%}
%\def\noninverseDeux{
%	\AXC{}
%	\RightLabel{$(id)$}
%	\UIC{$A\vdash A$}
%	\RightLabel{$(\1 L)$}
%	\UIC{$A,\1\vdash A$}
%	\RightLabel{$(\otimes L)$}
%	\UIC{$A\otimes\1\vdash A$}
%		\AXC{}
%		\RightLabel{$(\1 R)$}
%		\UIC{$\vdash \1$}
%	\RightLabel{$(\otimes R)$}
%	\BIC{$A\otimes\1\vdash A\otimes\1$}
%	\noLine
%	\UIC{}
%	\noLine
%	\UIC{$p_{2}$}
%\DP
%}
%\def\noninverseTrois{
%	\AXC{}
%	\RightLabel{$(id)$}
%	\UIC{$A\vdash A$}
%		\AXC{}
%		\RightLabel{$(id)$}
%		\UIC{$\1 \vdash \1$}
%	\RightLabel{$(\otimes R)$}
%	\BIC{$A,\1\vdash A\otimes\1$}
%	\RightLabel{$(\otimes L)$}
%	\UIC{$A\otimes\1\vdash A\otimes\1$}
%	\noLine
%	\UIC{}
%	\noLine
%	\UIC{$p_{3}$}
%\DP
%}
%\noindent
%\resizebox{\linewidth}{!}{
%%\begin{tabular}{ccc}
%%	\noninverseUn & \noninverseDeux & \noninverseTrois
%%\end{tabular}
%\noninverseUn \quad \noninverseDeux \quad \noninverseTrois
%}


%


\subsection{\'Echange et catégorie monoïdale symétrique}

\def\sizesymetrie{0.27\textwidth}
\begin{floatingfigure}[r]{\sizesymetrie}
\centering
\resizebox{\sizesymetrie}{!}{
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$B\vdash B$}
		\AXC{}
		\RightLabel{$(id)$}
		\UIC{$A\vdash A$}
	\RightLabel{$(\otimes R)$}
	\BIC{$B,A \vdash B\otimes A$}
	\RightLabel{$(exchange)$}
	\UIC{$A,B \vdash B\otimes A$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes B \vdash B\otimes A$}
\DP
}
\\
\end{floatingfigure}

La logique linéaire est commutative : la règle d'échange ... permet de construire la preuve canonique de $A\otimes B \vdash B\otimes A$ ci-contre. On peut alors ajouter un nouvel isomorphisme naturel à $\CProofs$ pour en faire une catégorie monoïdale symétrique. Des variantes ``non commutatives'' ont été étudiées dans la littérature, où la règle d'échange est supprimée ou affaiblie. %Dans ce dernier cas, on peut parfois obtenir une catégorie monoïdale non pas symétrique, mais \emph{tressée}
Dans ce dernier cas, on peut parfois obtenir quand même une catégorie monoïdale tressée.

\vspace{3mm}
%\

\begin{df}
Soit $\Ccal$ une catégorie monoïdale, avec les notations précédentes. C'est une \textbf{\emph{catégorie monoïdale tressée}} s'il existe un \textbf{isomorphisme naturel} $\gamma_{A,B} : A\otimes B \to B\otimes A$ tel que les \emph{diagrammes hexagonaux} suivants commutent.
\vspace{1mm}

%\noindent
%\includegraphics[width=0.48\linewidth]{diagrammeHexa1}
%\quad
%\includegraphics[width=0.48\linewidth]{diagrammeHexa2}

{\centering
%\resizebox{0.7\linewidth}{!}{
\begin{tikzcd}
	& A\otimes(B\otimes C)
		\arrow[r,"\gamma_{A,B\otimes C}"]
	& (B\otimes C)\otimes A
		\arrow[rd,bend left,"\alpha_{B,C,A}"]
& \\
	(A\otimes B)\otimes C
		\arrow[ru,bend left,"\alpha_{A,B,C}"]
		\arrow[rd,bend right,"\gamma_{A,B}\otimes id_{C}"']	
	& & & B\otimes(C\otimes A)
\\
	& (B\otimes A)\otimes C
		\arrow[r,"\alpha_{B,A,C}"]
	& B\otimes(A\otimes C)
		\arrow[ru,bend right,"id_{B}\otimes\gamma_{A,C}"']
\end{tikzcd}
\quad

\noindent
\begin{tikzcd}
	& (A\otimes B)\otimes C
		\arrow[r,"\gamma_{A\otimes B,C}"]
	& C\otimes(A\otimes B)
		\arrow[rd,bend left,"\alpha^{-1}_{C,A,B}"]
& \\
	A\otimes(B\otimes C)
		\arrow[ru,bend left,"\alpha^{-1}_{A,B,C}"]
		\arrow[rd,bend right,"id_{A}\otimes\gamma_{B,C}"']	
	& & & (C\otimes A)\otimes B
\\
	& A\otimes(C\otimes B)
		\arrow[r,"\alpha^{-1}_{A,C,B}"]
	& (A\otimes C)\otimes B
		\arrow[ru,bend right,"\gamma_{C,A}\otimes id_{B}"']
\end{tikzcd}
%}

}
\end{df}



\begin{df}
Soit $\Ccal$ une catégorie monoïdale tressée, avec les notations précédentes. C'est une \textbf{\emph{catégorie monoïdale symétrique}} si pour tous objets $A$, $B$, on a $\gamma_{B,A}=\gamma_{A,B}^{-1}$. Dans ce cas, on n'a pas besoin de vérifier la commutativité du second diagramme de la définition précédente, qui est entraînée par la commutativité du premier diagramme appliquée à $\gamma_{B,A}$.
\end{df}

Dans $\Ccal$, on définit $\gamma_{A,B}$ comme la dénotation de la preuve canonique de $A\otimes B \vdash B\otimes A$ donnée précédemment. %Si on a assoupli l'équivalence d'élimination de la coupure pour que $\Ccal$ soit une catégorie monoïdale, on 
On obtient alors une \textbf{catégorie monoïdale symétrique}.
\idees{vrai ? démonstrations, explications, exemples ?}


%



\subsection{Implication linéaire et catégorie monoïdale fermée}

Intéressons-nous maintenant à l'implication linéaire $\multimap$. Ce connecteur logique induit encore une fois un opérateur sur les dénotations de formules, qui a un sens en théorie des catégories.

\begin{df}
Soit $\Ccal$ une catégorie monoïdale, toujours avec les mêmes notations. Une \textbf{\emph{structure fermée à gauche}} sur $\Ccal$ consiste en un opérateur $\multimap$ sur les objets et pour tous objets $A$, $B$, un morphisme $eval_{A,B} : A\otimes(A\multimap B)\to B$, vérifiant la propriété d'universalité suivante : pour tout objet $X$ et tout morphisme $f:A\otimes X\to B$, il existe un unique morphisme $h:X\to A\multimap B$ tel que le diagramme suivant commute.

\noindent
{\centering
%\includegraphics[width=4cm]{diagrammeFermeeGauche}
\begin{tikzcd}
	A\otimes X
		\arrow[d,"id_{A}\otimes h"']
		\arrow[rd,"f"]
& \\
	A\otimes(A\multimap B)
		\arrow[r,"eval_{A,B}"']
	& B
\end{tikzcd}

}
On dit alors que $\Ccal$ est une \textbf{\emph{catégorie monoïdale fermée}}.
\end{df}


\def\sizesymetrie{0.3\textwidth}
\begin{floatingfigure}[r]{\sizesymetrie}
\centering
\resizebox{\sizesymetrie}{!}{
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
		\AXC{}
		\RightLabel{$(id)$}
		\UIC{$B\vdash B$}	
	\RightLabel{$(\multimap L)$}
	\BIC{$A,(A\multimap B) \vdash B$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes(A\multimap B) \vdash B$}
\DP
}
\\
\end{floatingfigure}

On munit $\CProofs$ de l'opérateur $\multimap$ sur les dénotations de formules correspondant à l'implication linéaire. On définit $eval_{A,B}$ comme la dénotation de la preuve ci-contre. On obtient alors une \textbf{catégorie monoïdale fermée}.
\idees{Est-ce vrai ? Est-ce qu'on peut expliquer facilement comment obtenir $h$ à partir de $f$ ?}

\

\

\idees{généralisation à n'importe quelle preuve de logique intuitionniste ?}

%%%

\section*{Conclusion}




%%%%%%%%%%%%%%

%\end{document}
\begin{comment}





\subsection{``Avec'' et produit cartésien}


\subsection{Catégorie libre}







\pagebreak





% def catégorie
\begin{df}
Une \textbf{\emph{catégorie}} consiste en une collection d'\emph{objets} et une collection de \emph{morphismes}, cette dernière munie d'une opérations binaire partielle $\circ$ appelée \emph{composition}, avec les propriétés suivantes.

\begin{itemize}

\item
\`A chaque morphisme $f$ est associé un couple d'objets $(A,B)$ ; on écrit $f:A\to B$.
On dit que $A\to B$ est le \emph{type} de $f$, et que $f$ est un morphisme \emph{de} $A$ \emph{vers} $B$, et encore que $A$ est le \emph{domaine} ou la \emph{source} de $f$, et $B$ le \emph{codomaine} ou la \emph{cible} de $f$.

\item
Pour tous objets $A$, $B$, $C$ et morphismes $f$, $g$ tels que $f:A\to B$ et $g:B\to C$, le morphisme $g\circ f$ existe et $g\circ f:A\to C$.

\item
\textbf{Identité.}
Pour tout objet $A$, il existe un morphisme particulier $id_{A}:A\to A$ appelé \emph{identitié sur $A$}, tel que 
%pour tous objet $B$ et morphismes $f:B\to A$ et $g:A\to B$, $id_{A}\circ f=f$ et $g\circ id_{A}=g$.
pour tout objet $B$, pour tout $f:B\to A$, $id_{A}\circ f=f$ et pour tout $g:A\to B$, $g\circ id_{A}=g$.

\item
\textbf{Associativité.}
Pour tous $f:A\to B$, $g:B\to C$, $h:C\to D$, on a $h\circ(g\circ f) = (h\circ g)\circ f$.

\end{itemize}
\end{df}

On désigne souvent une catégorie par la collection de ses objets.


% def catégorie monoïdale
\begin{df}
%Une \textbf{\emph{catégorie monoïdale}} est une catégorie $\C$ munie d'un bifoncteur ${\otimes:\C\times\C \longrightarrow \C}$ tel que
%
%\begin{itemize}
%%\item
%%pour tous objets $A$, $B$, $C$, il existe une isomorphisme naturel d'associativité
%%\linebreak
%%${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$ ;
%%\item
%%il existe un objet $e$, unitaire
%\item
%une \textbf{associativité} est fournie par un morphisme naturel pour tous objets $A$, $B$, $C$ : ${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$ ;
%\item
%il existe un objet $e$ \textbf{unitaire} grâce à des morphismes naturels pour tout objet $A$ : ${\lambda_{A}:e\otimes A\to A}$ et ${\rho_{A}:A\otimes e\to A}$.
% 
%\end{itemize}

Une \textbf{\emph{catégorie monoïdale}} est une catégorie $\Ccal$ munie d'un bifoncteur ${\otimes:\Ccal\times\Ccal \longrightarrow \Ccal}$ et d'un objet $e$, tel que les morphismes suivants existent

\begin{itemize}
\item
pour tous objets $A$, $B$, $C$, un isomorphisme
%\linebreak
${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$ permettant de parler d'associativité ;
\item
pour tout objet $A$, des isomorphismes ${\lambda_{A}:e\otimes A\to A}$ et ${\rho_{A}:A\otimes e\to A}$ justifiant le nom d'\textbf{unité} pour $e$ ;
\end{itemize}

\noindent
et tel que les diagrammes suivants commutent pour tous objets $A$, $B$, $C$, $D$. On a omis les indices de $\alpha$, $\lambda$, $\rho$ et écrit $A$ pour $id_{A}$ : par exemple, comprendre $\alpha\otimes D$ comme $\alpha_{A,B,C}\otimes id_{A}$.

\includegraphics[width=\linewidth]{diagrammeMonoidalePenta}
\centering{\includegraphics[width=0.5\linewidth]{diagrammeMonoidaleTri}}

\end{df}





\pagebreak

\`A chaque preuve $\pi$ on associe une \emph{dénotation} $[\pi]$, qu'on veut invariante par élimination de la coupure.

\

Les objets sont les formules et les morphismes sont des dénotations de preuves. Les morphismes d'une formule $A$ vers une formule $B$ sont les dénotations des différentes preuves du séquent $A\vdash B$ (si le séquent n'est pas prouvable, il n'y en a pas).

Soit $A$, $B$, $C$ des formules, $\pi_{1}$ une preuve de $A\vdash B$ et $\pi_{2}$ une preuve de $B\vdash C$. On définit $[\pi_{2}]\circ[\pi_{1}]$ comme la dénotation de la preuve suivante de $A\vdash C$
\begin{prooftree}
	\AXC	{$\pi_{1}$}
	\noLine
	\UIC{$A\vdash B$}
	\AXC	{$\pi_{2}$}
	\noLine
	\UIC{$B\vdash C$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash C$}
\end{prooftree}

L'identité $id_{A}$ sur une formule $A$ est la dénotation de la preuve
{\centerAlignProof
	\AXC	{}
	\RightLabel{$(Id)$}
	\UIC{$A\vdash A$}
	\DP
}.
Soit $\pi$ une preuve de $A\vdash B$, l'élimination de la coupure transforme la preuve
\\
\begin{center}
	\AXC	{}
	\RightLabel{$(Id)$}
	\UIC{$A\vdash A$}
	\AXC	{$\pi$}
	\noLine
	\UIC{$A\vdash B$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash B$}
	\DP
\qquad\qquad en \qquad\qquad
	\AXC	{$\pi$}
	\noLine
	\UIC{$A\vdash B$}
	\DP
\end{center}

\noindent
donc on a bien $[\pi]\circ id_{A}=[\pi]$. De même pour $\pi$ une preuve de $B\vdash A$, on a $id_{A}\circ[\pi]=[\pi]$.

L'associativité vient de ce que les preuves
\\
\begin{center}
	\AXC	{$\pi_{1}$}
	\noLine
	\UIC{$A\vdash B$}
	\AXC	{$\pi_{2}$}
	\noLine
	\UIC{$B\vdash C$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash C$}
	\AXC	{$\pi_{3}$}
	\noLine
	\UIC{$C\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash D$}
	\DP
\quad et \quad
	\AXC	{$\pi_{1}$}
	\noLine
	\UIC{$A\vdash B$}
	\AXC	{$\pi_{2}$}
	\noLine
	\UIC{$B\vdash C$}
	\AXC	{$\pi_{3}$}
	\noLine
	\UIC{$C\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$B\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash D$}
	\DP
\end{center}

\noindent
sont équivalentes à élimination de la coupure près.






\end{comment}

%\end{document}
