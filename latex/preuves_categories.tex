%\input{preamble}
%\begin{document}
%\renewcommand{\labelitemi}{$\bullet$}
%\vspace{2cm}~~\\
%\input{bussproofs}



%communs : id,coupure
\def\id{
\centerAlignProof
	\AXC	{}
	\RightLabel{$(id)$}
	\UIC{$A \vdash A$}
	\DP
}
\def\cut{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G',A \vdash \D'$}
	\RightLabel{$(cut)$}
	\BIC{$\G,\G' \vdash \D,\D'$}
	\DP
}


%\vspace*{-3em}
\vspace*{0em}


\part*{Mémoire : Calcul des séquents \\et transfert vers une catégorie}

\vspace{1em}

%\idees{Rapport de stage : page \pageref{part:rapportstage}}

\section*{Introduction}

Une ``preuve'' évoque souvent une succession d'arguments
%, que ce soit dans un livre, où l'objectif est de convaincre le lecteur de la validité d'une propriété énoncée, ou au tableau, le destinataire étant cette fois un auditoire. 
dont l'objectif est de convaincre un auditoire ou un lecteur de la validité d'une assertion.
La \emph{théorie de la preuve} formalise, à l'aide d'un langage mathématique, cet événement consistant à convaincre quelqu'un : une \emph{preuve} devient alors un objet mathématique. 
%Elle se situe au croisement entre mathématiques et informatique.
Il s'agit d'une branche de la logique %située au croisement entre mathématiques et informatique, qui est également étudiée en philosophie.
qui s'intègre à la fois en mathématiques, en informatique et en philosophie.
%
%Un des formalismes 
Le calcul des séquents, introduit par Gentzen, est un formalisme de théorie de la preuve qui fait l'objet de nombreuses études. Il présente un grand intérêt en informatique, puisqu'on peut en déduire assez naturellement un algorithme de recherche de preuves, bien qu'il faille prendre certaines précautions pour que cet algorithme termine. Ceci est mis en application dans mon stage. J'ai choisi pour ce mémoire de m'intéresser à 
%une approche complètement différente 
un aspect complètement différent
du calcul des séquents : la construction, à partir d'un calcul des séquents pour la logique linéaire intuitionniste, d'une catégorie mathématique présentant de nombreuses propriétés. Cette construction s'appuie sur une preuve constructive d'un théorème fondamental en calcul des séquents : le théorème d'élimination de la coupure. 
%Curieusement, ce théorème présente en recherche automatisée de preuves un intérêt
On retrouve ce théorème en recherche automatisée de preuves, où il présente un intérêt décisif qui consiste à l'oublier ! En effet, il permet de se débarrasser d'une règle de coupure dont la présence est catastrophique pour la terminaison d'un algorithme basé sur un calcul des séquents.


%th élimination de la coupure, %bien utile 
%dont le seul mais immense intérêt
%en recherche automatisée de preuves %pour 
%est de se débarrasser d'une règle de coupure dont la présence est catastrophique pour la terminaison d'un algorithme basé sur un calcul des séquents.


Dans une première partie, on présente le calcul des séquents, en s'appuyant sur l'exemple du calcul \LK\ correspondant à la logique classique ; on explique aussi comment Gentzen en dérive un autre calcul \LJ\ correspondant à la logique intuitionniste, une logique constructive qu'on peut obtenir à partir de la logique classique en supprimant l'axiome du tiers exclu $A\lor\lnot A$. On introduit ensuite la logique linéaire puis la logique linéaire intuitionniste (ILL), présentée à travers des calculs de séquents qui leur correspondent. On expose alors le principe d'une démonstration constructive du théorème d'élimination de la coupure pour ILL, ce qui définit un \emph{procédé d'élimination de la coupure}. Enfin, on s'appuie sur ce dernier afin de construire une catégorie correspondant au calcul des séquents pour ILL, sur laquelle on énonce quelques propriétés mathématiques.


\setcounter{tocdepth}{1}
%\tableofcontents
\startlist{toc}
\printlist{toc}{}{\section*{Table des matières}}

\

\


%%%

\section{Introduction aux calculs des séquents à travers \LK\ pour la logique classique et \LJ\ pour la logique intuitionniste}
\label{section:introductionCalculsSequents}



%\idees{
%calculs des séquents :
%outils / procédé de raisonnement
%... dans l'introduction générale du mémoire ?
%}

Nous introduisons dans cette première partie les définitions dont nous aurons besoin sur les calculs des séquents, à travers l'exemple du calcul des séquents \LK. Nous expliquons en quoi ce calcul défini par Gentzen correspond à la logique classique. Nous présentons ensuite le calcul \LJ, que Gentzen a dérivé de \LK\ afin de représenter la logique intuitionniste. Enfin, nous expliquons comment certaines propriétés des deux logiques considérées peuvent se comprendre en étudiant leur calcul des séquents respectif.

Les formules considérées dans cette partie sont construites à partir de constantes $\bot$ (\emph{faux}) et $\top$ (\emph{vrai}), de variables propositionnelles, du connecteur unaire $\lnot$ (\emph{non}), et des connecteurs binaires $\land$ (\emph{et}), $\lor$ (\emph{ou}) et $\to$ (\emph{implique}). On s'intéresse en effet aux parties propositionnelles de ces logiques.


\subsection{Des séquents et des règles}

Un calcul des séquents se caractérise par sa propre définition d'un objet syntaxique appelé \emph{séquent}, ainsi que par un ensemble de \emph{règles} agissant sur les séquents.

Par exemple, pour le calcul des séquents \LK, la définition d'un séquent est la suivante, et les règles sont données dans la figure~\ref{fig:reglesLK}.

\begin{df}
Un \textbf{\emph{séquent}} de \LK\ consiste en deux listes de formules $\G$ (les ``hypothèses'') et $\D$ (les ``conclusions'') ; on le note $\G \vdash \D$. On appellera \emph{séquent classique} un tel séquent.
\end{df}


\noindent
\textit{Notation.} $X,Y$ désigne la liste obtenue en concaténant les listes $X$ et $Y$ ; si $X$ ou $Y$ est une formule, on la considère comme la liste à un élément correspondante.

%LK
%id, coupure : communs

%LK,règles logiques
\def\LKfauxL{
	\AXC	{}
	\RightLabel{$(\bot L)$}
	\UIC{$\G,\bot \vdash \D$}
	\DP
}
\def\LKvraiR{
	\AXC	{}
	\RightLabel{$(\top R)$}
	\UIC{$\G \vdash \top,\D$}
	\DP
}
\def\LKnonL{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(\lnot L)$}
	\UIC{$\G,\lnot A \vdash \D$}
	\DP
}
\def\LKnonR{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(\lnot L)$}
	\UIC{$\G \vdash \lnot A,\D$}
	\DP
}
\def\LKetL{
	\AXC	{$\G,A,B \vdash \D$}
	\RightLabel{$(\land L)$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP
}
\def\LKetR{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G \vdash B,\D$}
	\RightLabel{$(\land R)$}
	\BIC{$\G \vdash A\land B,\D$}
	\DP
}
\def\LKouL{
	\AXC	{$\G,A \vdash \D$}
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\lor L)$}
	\BIC{$\G,A\lor B \vdash \D$}
	\DP
}
\def\LKouR{
	\AXC	{$\G \vdash A,B,\D$}
	\RightLabel{$(\lor R)$}
	\UIC{$\G \vdash A\lor B,\D$}
	\DP
}
\def\LKimpL{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\to L)$}
	\BIC{$\G,A\to B \vdash \D$}
	\DP
}
\def\LKimpR{
	\AXC	{$\G,A \vdash B,\D$}
	\RightLabel{$(\to R)$}
	\UIC{$\G \vdash A\to B,\D$}
	\DP
}
%LK, règles structurelles
\def\LKweakeningL{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(weakening\ L)$}
	\UIC{$\G,A \vdash \D$}
	\DP
}
\def\LKweakeningR{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(weakening\ R)$}
	\UIC{$\G \vdash A,\D$}
	\DP
}
\def\LKcontractionL{
	\AXC	{$\G,A,A \vdash \D$}
	\RightLabel{$(contraction\ L)$}
	\UIC{$\G,A \vdash \D$}
	\DP
}
\def\LKcontractionR{
	\AXC	{$\G \vdash A,A,\D$}
	\RightLabel{$(contraction\ R)$}
	\UIC{$\G \vdash A,\D$}
	\DP
}
\def\LKexchangeL{
	\AXC	{$\G_{1},A,B,\G_{2} \vdash \D$}
	\RightLabel{$(exchange\ L)$}
	\UIC{$\G_{1},B,A,\G_{2} \vdash \D$}
	\DP
}
\def\LKexchangeR{
	\AXC	{$\G \vdash \D_{1},A,B,\D_{2}$}
	\RightLabel{$(exchange\ L)$}
	\UIC{$\G \vdash \D_{1},B,A,\D_{2}$}
	\DP
}

%LK variantes
\def\LKetLun{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(\land L_{1})$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP
}
\def\LKetLdeux{
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\land L_{2})$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP
}

\def\LKouRun{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(\lor R_{1})$}
	\UIC{$\G \vdash A\lor B,\D$}
	\DP
}
\def\LKouRdeux{
	\AXC	{$\G \vdash B,\D$}
	\RightLabel{$(\lor R_{2})$}
	\UIC{$\G \vdash A\lor B,\D$}
	\DP
}


\begin{figure}[h]
%\begin{floatingfigure}[r]{0.6\linewidth}
\centering
{\renewcommand{\arraystretch}{1.3}
\resizebox{0.6\linewidth}{!}{
%$\begin{tabular}{|cc|c|}
%	\hline
%	\textbf{\large{Identité}} & \LJid & \textbf{\large{Coupure}} \\
%	\cline{1-2}
%	\LJbotL & \textbf{\large{Règles logiques}} & \LJcut \\[4pt]
%	\cline{3-3}
%	\LJetL & \LJetR & \textbf{\large{Règles structurelles}} \\
%	\LJouL & \LJouRun \LJouRdeux & \LJweakening \\
%	\LJimpL & \LJimpR & \LJcontraction \\[4pt]
%	\hline
%\end{tabular}$
$\begin{tabular}{|cc|}
	\hline
	\multicolumn{1}{|c|}{\textbf{\large{Identité}}} & \textbf{\large{Coupure}} \\ 
	\multicolumn{1}{|c|}{\id} & \cut \\[9pt] \hline
	\multicolumn{2}{|c|}{\textbf{\large{Règles logiques}}} \\
	\LKfauxL & \LKvraiR \\[5pt]
	\LKnonL & \LKnonR \\[9pt]
	\LKetL & \LKetR \\[9pt]
	\LKouL & \LKouR \\[9pt]
	\LKimpL & \LKimpR \\[9pt] \hline
	\multicolumn{2}{|c|}{\textbf{\large{Règles structurelles}}} \\
	\LKweakeningL & \LKweakeningR \\[9pt]
	\LKcontractionL & \LKcontractionR \\[9pt]
	\LKexchangeL & \LKexchangeR \\[9pt] \hline
%	\LKL & \LKR \\
\end{tabular}$
}
}
\caption{Règles du calcul \LK}
\label{fig:reglesLK}
\end{figure}
%\end{floatingfigure}


Pour une règle ${\regle,}$ $\mathcal R$ est le nom de la règle, $prem_{1}$, ... , $prem_{p}$ sont les \textbf{\emph{prémisses}}, et $concl$ la \textbf{\emph{conclusion}}. 
%$prem_{k}$ sera appelée la \emph{$k$-ième prémisse} ou \emph{prémisse numéro $k$}.%?
Les prémisses et la conclusion sont des séquents où $A$, $B$ sont des formules quelconques et $\G$, $\D$ des listes de formules quelconques. 
L'idée est qu'une règle affirme : si toutes les prémisses sont vraies, alors la conclusion est aussi vraie. Cette idée est formalisée en \ref{subsection:ProuvabiliteSequent}.
%Les \textbf{\emph{axiomes}} sont les règles sans prémisse.%?

On distingue deux grandes familles de règles. Les \textbf{\emph{règles logiques}} remplacent une formule de la conclusion par une ou des formules plus simples. La formule remplacée, appelée \emph{formule principale}, doit avoir une forme donnée en fonction de la règle. Les  \textbf{\emph{règles structurelles}} manipulent la structure du séquent en enlevant, dupliquant, dépla\c cant des formules dont on n'a pas besoin de connaître la forme. Elles dépendent du choix de structure du séquent : 
par exemple, si on représentait $\G$ et $\D$ par des \emph{multiensembles}, c'est-à-dire des collections où le nombre d'occurrences est pris en compte mais pas l'ordre des éléments, on n'aurait pas besoin des règles d'échange $exchange\ L$ et $exchange\ R$.

Ce qui distingue notamment le calcul des séquents d'un système de déduction naturelle est que toutes les règles logiques sont des règles d'\emph{introduction à gauche} ou \emph{à droite}, c'est-à-dire que la conclusion contient une constante ou un connecteur supplémentaire par rapport aux prémisses. En déduction naturelle, on aurait des règles d'\emph{élimination} comme
	\AXC	{$\; \vdash A\land B$}
	\UIC{$\; \vdash A$}
	\DP
.


\subsection{Variantes d'écriture de certaines règles}
\label{subsection:VariantesEcritureRegles}

Cette présentation %, inspirée de la présentation de \LJ\ par Dyckhoff dans \cite{LJT}, 
diffère %légèrement 
de celle de Gentzen, mais elle en est suffisamment proche pour qu'on puisse quand même appeler ce calcul des séquents \LK. Gentzen écrit deux règles\LKetLun et \LKetLdeux à la place de \LKetL, et de même deux autres règles à la place de $\lor R$. On a cependant une équivalence grâce aux règles structurelles. Ci-dessous à gauche, on retrouve en effet la règle $\land L_{1}$ à partir de $\land L$ et $weakening\ L$, et on peut faire de même pour $\land L_{2}$. \`A droite, on retrouve $\land L$ à partir de $\land L_{1}$ et $\land L_{2}$ et $contraction\ L$. On s'autorise à faire agir $\land L_{1}$ sur une formule qui n'est pas la dernière de la liste : cela est possible en appliquant plusieurs fois la règle $exchange\ L$, 
ce qu'on n'a pas fait explicitement par souci de lisibilité.
%mais le faire explicitement alourdirait 
\vspace{1mm}

{
\centering
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(weakening L)$}
	\UIC{$\G,A,B \vdash \D$}
	\RightLabel{$(\land L)$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP
\qquad
	\AXC	{$\G,A,B \vdash \D$}
	\RightLabel{$(\land L_{1})$}
	\UIC{$\G,A\land B,B \vdash \D$}
	\RightLabel{$(\land L_{2})$}
	\UIC{$\G,A\land B,A\land B \vdash \D$}
	\RightLabel{$(contraction L)$}
	\UIC{$\G,A\land B \vdash \D$}
	\DP

}
%	\AXC	{$\G,A \vdash \D$}
%	\RightLabel{$(\land L_{1})$}
%	\UIC{$\G,A\land B \vdash \D$}
%	\DP




\subsection{Prouvabilité d'un séquent}
\label{subsection:ProuvabiliteSequent}

%Les définitions suivantes s'appliquent aux calculs des séquents en général, pas seulement \LJ.
Une \textbf{\emph{instance}} d'une règle $\mathcal R$ a la même forme que la règle : \instance, mais ici les $\s_{i}$ et $\s$ sont des séquents connus explicitement ; bien entendu il faut qu'il s'agisse des séquents qui correspondent à la forme donnée par la définition de la règle. %Par exemple \etL\ devient une instance de la règle $\land L$ (qui a la même écriture que la règle) lorsqu'on connaît les formules $A$ et $B$ et toutes les formules de $\Th$, $\G$, $\D$.
Une \textbf{\emph{preuve}} (ou \textbf{\emph{arbre de preuve}}) est un arbre dont les n\oe uds sont étiquetés par un séquent et une règle et ont la même arité que le nombre de prémisses de la règle, et tel que : pour tout n\oe ud de séquent $\s$ et de règle $\mathcal R$, si $\s_{1}$, ... , $\s_{p}$ sont les séquents associés à chacun de ses fils respectivement, alors\instance\ est une instance de $\mathcal R$. Les feuilles d'un tel arbre sont les n\oe uds auxquels est associé un axiome.

\begin{df}
Un séquent $\s$ est \textbf{\emph{prouvable}} \emph{dans} 
%(ou \emph{par}) 
%ou \emph{par}
\emph{un calcul des séquents} s'il existe un arbre de preuve tel que le séquent associé à la racine est $\s$. De manière équivalente, on peut définir l'ensemble des séquents prouvables comme le plus petit ensemble vérifiant : pour toute instance \instance\ d'une règle, si pour tout $i$, $\s_{i}$ est prouvable, alors $\s$ est prouvable (en particulier pour toute instance \instanceAx\ d'un axiome $\mathcal A$, $\s$ est prouvable).
\end{df}


\subsection{Lien avec une logique, interprétation des séquents}

Un calcul des séquents est généralement associé à une logique, à travers une propriété similaire à la suivante, qui concerne \LK\ et la logique classique.

\begin{prop}
Une formule $A$ est valide en logique classique si, et seulement si, le séquent $\;\vdash A$ est prouvable par le calcul \LK\ (on écrit $\;\vdash A$ pour $\emptyset \vdash A$, $\emptyset$ désignant ici la liste vide).
\end{prop}

Les séquents sont des objets syntaxiques pratiques à manipuler à l'aide de règles. Cependant, ils ont souvent une interprétation dans la logique considérée : par exemple pour \LK, un séquent $\G \vdash \D$ s'interprète comme une formule de logique classique grâce à la propriété suivante. Il signifie ainsi : ``si on suppose toutes les formules de $\G$, on peut montrer au moins une formule de $\D$'', d'où
%De là viennent 
les appellations ``hypothèses'' pour les formules de $\G$ et ``conclusions'' pour celles de $\D$.

\begin{prop}
Un séquent $\G \vdash \D$ est prouvable par le calcul \LK\ si, et seulement si, la formule $\left(\bigwedge_{G\in\G}G\right)\to\left(\bigvee_{D\in\D}D\right)$ est valide en logique classique.
\end{prop}

\subsection{Calcul des séquents \LJ\ et logique intuitionniste}

Gentzen a dérivé le calcul \LJ\ de \LK\ dans le but de correspondre à la logique intuitionniste. La modification apportée à \LK\ pour cela est simple et élégante : on restreint les séquents à ceux qui ont exactement une formule à droite ; les règles sont adaptées en conséquence. La principale conséquence de cette restriction est l'impossibilité d'utiliser les règles structurelles à droite du séquent. La nouvelle définition d'un séquent est donc la suivante, et les règles sont données dans la figure~\ref{fig:reglesLJ}. 

\begin{df}
Un \textbf{\emph{séquent}} de \LJ\ consiste en une liste de formules $\G$ (les ``hypothèses'') et une formule $D$ (la ``conclusion'') ; on le note $\G \vdash D$. On appelera \emph{séquent intuitionniste} un tel séquent.
\end{df}


%LJ
%id : commun
%cut
\def\LJcut{
	\AXC	{$\G \vdash A$}
	\AXC	{$\G',A \vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$\G,\G' \vdash D$}
	\DP
}

%LJ,règles logiques
\def\LJfauxL{
	\AXC	{}
	\RightLabel{$(\bot L)$}
	\UIC{$\G,\bot \vdash D$}
	\DP
}
\def\LJetL{
	\AXC	{$\G,A,B \vdash D$}
	\RightLabel{$(\land L)$}
	\UIC{$\G,A\land B \vdash D$}
	\DP
}
\def\LJetR{
	\AXC	{$\G \vdash A$}
	\AXC	{$\G \vdash B$}
	\RightLabel{$(\land R)$}
	\BIC{$\G \vdash A\land B$}
	\DP
}
\def\LJouL{
	\AXC	{$\G,A \vdash D$}
	\AXC	{$\G,B \vdash D$}
	\RightLabel{$(\lor L)$}
	\BIC{$\G,A\lor B \vdash D$}
	\DP
}
\def\LJouRun{
	\AXC	{$\G \vdash A$}
	\RightLabel{$(\lor R_{1})$}
	\UIC{$\G \vdash A\lor B$}
	\DP
}
\def\LJouRdeux{
	\AXC	{$\G \vdash B$}
	\RightLabel{$(\lor R_{2})$}
	\UIC{$\G \vdash A\lor B$}
	\DP
}
\def\LJimpL{
	\AXC	{$\G \vdash A$}
	\AXC	{$\G,B \vdash D$}
	\RightLabel{$(\to L)$}
	\BIC{$\G,A\to B \vdash D$}
	\DP
}
\def\LJimpR{
	\AXC	{$\G,A \vdash B$}
	\RightLabel{$(\to R)$}
	\UIC{$\G \vdash A\to B$}
	\DP
}
%LJ, règles structurelles
\def\LJcontractionL{
	\AXC	{$\G,A,A \vdash D$}
	\RightLabel{$(contraction\ L)$}
	\UIC{$\G,A \vdash D$}
	\DP
}
\def\LJweakeningL{
	\AXC	{$\G \vdash D$}
	\RightLabel{$(weakening\ L)$}
	\UIC{$\G,A \vdash D$}
	\DP
}
\def\LJexchangeL{
	\AXC	{$\G_{1},A,B,\G_{2} \vdash D$}
	\RightLabel{$(exchange\ L)$}
	\UIC{$\G_{1},B,A,\G_{2} \vdash D$}
	\DP
}


\begin{figure}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\resizebox{0.7\linewidth}{!}{
$\begin{tabular}{|cc|}
	\hline 
	\multicolumn{1}{|c|}{\textbf{\large{Identité}}} & \textbf{\large{Coupure}} \\
	\multicolumn{1}{|c|}{\id} & \LJcut \\[7pt] \hline
	\LJfauxL & \textbf{\large{Règles logiques}} \\[5pt]
	\LJetL & \LJetR \\[9pt]
	\LJouL & \LJouRun \LJouRdeux \\[9pt]
	\LJimpL & \LJimpR \\[7pt] \hline
%	\multicolumn{2}{|c|}{\textbf{\large{Règles structurelles}}} \\ \hline
	\textbf{\large{Règles structurelles}} & \LJweakeningL \\[9pt]
	\LJcontractionL & \LJexchangeL \\[7pt] \hline
\end{tabular}$
}
\caption{Règles du calcul \LJ}
\label{fig:reglesLJ}
\end{figure}


Les règles structurelles à droite de \LK, qui changent le nombre de formules à droite ou en nécessitent au moins deux, disparaissent. La règle\LKouR ne s'adapte pas immédiatement aux séquents de \LJ\ ; on la remplace par deux règles.%\ref{subsection:VariantesEcritureRegles}.

Le connecteur $\lnot$ disparaît également.  En effet, les règles qui lui sont associées dans \LK\ modifient le nombre de formules à droite du séquent. On donne bien un sens à $\lnot A$ en  logique intuitionniste, mais on considère qu'il s'agit d'une simple notation pour $A\!\to\!\bot$. 
De plus, en logique intuitionniste, $\lnot$ n'est pas involutif : la formule $\lnot\lnot A$ n'est pas équivalente à $A$.
%Cette décision est justifiée par le fait qu'en logique intuitionniste, $\lnot$ n'est pas involutif : la formule $\lnot\lnot A$ n'est pas équivalente à $A$.

\

Comme \LK\ avec la logique classique, \LJ\ est liée à la logique intuitionniste : \LJ\ permet de caractériser la prouvabilité d'une formule en logique intuitionniste. Un séquent de \LJ\ peut également s'interpréter en logique intuitionniste.

\begin{prop}
Une formule $A$ est prouvable en logique intuitionniste si, et seulement si, le séquent $\;\vdash A$ est prouvable par le calcul \LJ.
\end{prop}

\begin{prop}
Un séquent $\G \vdash D$ est prouvable par le calcul \LJ\ si, et seulement si, la formule $\left(\bigwedge_{G\in\G}G\right)\to D$ est prouvable en logique intuitionniste.
\end{prop}


%

\subsection{Une différence entre logique classique et logique intuitionniste}

\begin{floatingfigure}[r]{4.2cm}
\centering
	\AXC	{}
	\RightLabel{(id)}
	\UIC	{$A \vdash A$}
	\RightLabel{(weak.\ R)}
	\UIC	{$A \vdash A, \bot$}
	\RightLabel{$(\to R)$}
	\UIC	{$\vdash A\,,\; A\!\to\!\bot$}
	\RightLabel{$(\lor R)$}
	\UIC{$\vdash A\lor (A\!\to\!\bot)$}
	\DP
\\
%\caption{Preuve de $A\lor(A\!\to\!\bot)$ dans \LK}
%\label{fig:LKTiersExclu}
\end{floatingfigure}

C'est la possibilité d'avoir plusieurs formules dans la partie droite du séquent qui permet de prouver davantage des séquents dans \LK\ que dans \LJ. On comprend ainsi la différence entre le ``ou'' classique et le ``ou'' intuitionniste. En logique classique, prouver $A\lor B$, c'est prouver le séquent $\;\vdash A,B$ : les deux formules sont encore présentes. Un bon exemple est la preuve ci-contre dans \LK\ du principe du tiers exclu $A\lor\lnot A$ 
%(figure~\ref{fig:LKTiersExclu} ; on rappelle que $\lnot A$ est une notation pour $A\to\bot$) : 
, qu'on écrit $A\lor(A\!\to\!\bot)$ pour mieux comparer à la logique intuitionniste où le $\lnot$ se comporte différemment.
Si on peut appliquer l'axiome $id$ à la formule $A$ (ce qui nécessite deux occurrences distinctes de la formule, une de chaque côté), c'est bien parce qu'on a conservé les deux parties de la formule initiale. Tandis qu'en logique intuitionniste, pour prouver $A\lor B$ c'est-à-dire $\;\vdash A\lor B$, les seules règles applicables sont $\lor R_{1}$ et $\lor R_{2}$ (et la règle de coupure, mais on verra plus loin qu'elle n'est jamais nécessaire) : il faut donc prouver $\;\vdash A$ ou prouver $\;\vdash B$ ; une fois qu'on a choisi lequel on va prouver, on n'a plus accès à l'autre. Ainsi, on ne peut pas prouver $A\lor(A\!\to\!\bot)$, car ni le séquent $\;\vdash A$ ni le séquent $\;\vdash (A\!\to\!\bot)$ n'est prouvable.

\vspace{.3em}

\def\widthhere{7cm}
\begin{floatingfigure}[r]{\widthhere}
\centering
\resizebox{\widthhere}{!}{
	\AXC	{}
	\RightLabel{(id)}
	\UIC	{$A \vdash A$}
	\RightLabel{(weak.\ R)}
	\UIC	{$A \vdash A, \bot$}
	\RightLabel{$(\to R)$}
	\UIC	{$\vdash A\,,\; A\!\to\!\bot$}
	\RightLabel{$(\lor R_{2})$}
	\UIC	{$\vdash A\,,\; A\lor(A\to\bot)$}
	\RightLabel{$(\lor R_{1})$}
	\UIC{$\vdash A\lor (A\to\bot)\,,\;A\lor (A\to\bot)$}
	\RightLabel{$(contr.\ R)$}
	\UIC{$\vdash A\lor(A\!\to\!\bot)$}
	\DP
%	\AXC	{}
%	\RightLabel{(id)}
%	\UIC	{$A \vdash A, \bot$}
%	\RightLabel{$(\to R)$}
%	\UIC	{$\vdash A\,,\; (A\to\bot)$}
%	\RightLabel{$(\to R)$}
%	\UIC	{$\vdash A\,,\; A\lor(A\to\bot)$}
%	\RightLabel{$(\lor R_{1})$}
%	\UIC{$\vdash A\lor (A\to\bot)\,,\;A\lor (A\to\bot)$}
%	\RightLabel{$(contr.\ R)$}
%	\UIC{$\vdash A\lor (A\to\bot)$}
%	\DP
}
%\caption{Preuve de $A\lor(A\!\to\!\bot)$ dans \LK}
%\label{fig:LKTiersExclu}
\end{floatingfigure}

\noindent
\textit{Remarque.} %Même avec la présentation de \LK\ de Gentzen où on a deux règles\LKouRun et\LKouRdeux au lieu de notre unique règle $\lor R$, pour montrer 
Même avec la présentation de \LK\ de Gentzen où on a deux règles $\lor R_{1}$ et $\lor R_{2}$ au lieu de notre unique règle $\lor R$, dans la preuve ci-contre de $A\lor(A\!\to\!\bot)$, on utilise une contraction à droite afin de conserver à la fois $A$ et $A\!\to\!\bot$. Le principe est le même que dans les équivalences d'écriture des règles en \ref{subsection:VariantesEcritureRegles}.

\ %vspace{.3em}

Finalement, on peut considérer que la logique intuitionniste est obtenue à partir de la logique classique en interdisant certaines règles structurelles. Une autre logique couramment étudiée, la logique linéaire, s'obtient à partir de la logique classique en conservant toutes les règles structurelles, mais en restreignant fortement leur utilisation.

\



%%%



\section{Logique linéaire (LL) et logique linéaire intuitionniste (ILL)}

La logique linéaire (LL) est une extension de la logique classique. Elle présente un grand intérêt des points de vue logique aussi bien qu'informatique, notamment parce qu'elle contient une notion de ``ressources'' qui ne peuvent pas être utilisées inconsidérément. On peut considérer qu'elle est obtenue à partir de la logique classique en limitant l'usage des règles structurelles, si bien que de nouvelles nuances apparaissent, pour lesquelles sont introduits de nouveaux connecteurs. 

Voici un calcul des séquents pour la logique linéaire. Les séquents sont les mêmes que ceux de \LK\ : deux listes de formules $\G$ et $\D$, avec la notation $\G \vdash \D$. Les connecteurs, différents, sont donnés dans la figure~\ref{fig:connecteurs}, inspirée de (\cite{panorama}, p.40). 
Les formules sont alors construites à partir de variables propositionnelles, de ces connecteurs, et de constantes qui sont chacune un élément neutre associé à un connecteur. Par exemple $\1$ est l'élément neutre pour $\otimes$, c'est-à-dire que pour toute formule $A$, $A\otimes\1=\1\otimes A=A$.
%\`A chaque connecteur est associé un élément neutre, c'est-à-dire une constante
Les règles, empruntées à (\cite{troelstra}, Table~1), sont données dans la figure~\ref{fig:reglesLL}. Des explications sur ces connecteurs et ces règles sont proposées dans la sous-section~\ref{subsection:connecteursetc}, puis une interprétation en terme de ``ressources''~\cite{synsem} dans la sous-section~\ref{subsection:ressources}. Enfin, on présente la logique linéaire intuitionniste (ILL).%, obtenue à partir de la logique linéaire de la même manière que la logique intuitionniste est obtenue à partir de la logique classique.

%\idees{sources}

%La logique linéaire intuitionniste (ILL) est obtenue à partir de la logique linéaire de la même manière que la logique intuitionniste est obtenue à partir de la logique classique : en se restreignant aux séquents avec une unique formule à droite ; ici, 
%
%Ses séquents sont ainsi les même que ceux de
%
%On construit la logique linéaire intuitionniste (notée ILL) en restreignant encore une fois les séquents de la logique linéaire (notée LL) à des séquents avec une unique formule à droite.


\begin{figure}[h]
%\centering

%\def\nomCol{
%\resizebox{2.7cm}{!}{
%\begin{tabular}{cc}
%	\multicolumn{2}{c}{\'Elément neutre} \\
%	Symbole & Nom
%\end{tabular}
%}
%}

%\begin{tabular}{|c|c|c|c|}
%	\hline
%	\multirow{2}{*}{\large{Symbole}} & \multirow{2}{*}{\large{Nom}} & \multicolumn{2}{c|}{\small{\'Elément neutre}} \\[-1mm] 
%	& & \small{Symbole} & \small{Nom} \\ \hline
%	$\otimes$ & \emph{tenseur /} & $\1$ & \emph{un} \\[-1.5mm]
%	& \emph{produit tensoriel} & & \\ \hline
%	$\&$ & \emph{avec} & $\top$ & \emph{top} \\ \hline
%	$\oplus$ & \emph{plus} & $\0$ & \emph{zéro} \\ \hline
%	$\revAnd$ & \emph{produit parallèle} & $\bot$ & \emph{bot} \\ \hline
%	$\multimap$ & \emph{implication linéaire} & \multicolumn{2}{c|}{\scriptsize{pas d'élément neutre}} \\ \hline
%	\multicolumn{4}{c}{Connecteurs binaires}
%\end{tabular}
%\qquad\qquad
%\begin{tabular}{|c|c|}
%	\hline
%	\large{Symbole} & \large{Nom} \\ \hline
%	$\lnot$ & \emph{non} \\ \hline
%	$!$ & \emph{bang} \\ \hline
%	$?$ & \emph{why not} \\ \hline
%	\multicolumn{2}{c}{Connecteurs unaires}
%\end{tabular}
\begin{tabular}{|c|c|}
	\hline
	Symbole & Nom \\ \hline
	$\otimes$ & \emph{tenseur /} \\[-1.5mm]
	& \emph{produit tensoriel} \\ \hline
	$\&$ & \emph{avec} \\ \hline
	$\oplus$ & \emph{plus} \\ \hline
	$\revAnd$ & \emph{produit parallèle} \\ \hline
	$\multimap$ & \emph{implication linéaire} \\ \hline
	\multicolumn{2}{c}{Connecteurs binaires}
\end{tabular}
\quad
\begin{tabular}{|c|c|}
	\hline
	Symbole & Nom \\ \hline
	$\lnot$ & \emph{non} \\ \hline
	$!$ & \emph{bang} \\ \hline
	$?$ & \emph{why not} \\ \hline
	\multicolumn{2}{c}{Connecteurs unaires}
\end{tabular}
\quad
\begin{tabular}{|c|c|c|}
	\hline
	\multirow{2}{*}{Symbole} & \multirow{2}{*}{Nom} & \'Elément \\[-1mm] 
	& & neutre de \\ \hline
	$\1$ & \emph{un} & $\otimes$ \\ \hline
	$\top$ & \emph{top} & $\&$ \\ \hline
	$\0$ & \emph{zéro} & $\oplus$ \\ \hline
	$\bot$ & \emph{bot} & $\revAnd$ \\ \hline
	\multicolumn{3}{c}{Constantes}
\end{tabular}


\caption{Les connecteurs de la logique linéaire}
\label{fig:connecteurs}	
\end{figure}



%LL
%id, coupure : commun

%LL,règles logiques
\def\LLnonL{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(\lnot L)$}
	\UIC{$\G,\lnot A \vdash \D$}
	\DP
}
\def\LLnonR{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(\lnot R)$}
	\UIC{$\G \vdash \lnot A,\D$}
	\DP
}
\def\LLtenseurL{
	\AXC	{$\G,A,B \vdash \D$}
	\RightLabel{$(\otimes L)$}
	\UIC{$\G,A\otimes B \vdash \D$}
	\DP
}
\def\LLtenseurR{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G' \vdash B,\D'$}
	\RightLabel{$(\otimes R)$}
	\BIC{$\G \vdash A\otimes B,\D,\D'$}
	\DP
}
\def\LLavecLun{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(\& L_{1})$}
	\UIC{$\G,A\& B \vdash \D$}
	\DP
}
\def\LLavecLdeux{
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\& L_{2})$}
	\UIC{$\G,A\& B \vdash \D$}
	\DP
}
\def\LLavecR{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G \vdash B,\D$}
	\RightLabel{$(\& R)$}
	\BIC{$\G \vdash A\& B,\D$}
	\DP
}
\def\LLplusL{
	\AXC	{$\G,A \vdash \D$}
	\AXC	{$\G,B \vdash \D$}
	\RightLabel{$(\oplus L)$}
	\BIC{$\G,A\oplus B \vdash \D$}
	\DP
}
\def\LLplusRun{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(\oplus R_{1})$}
	\UIC{$\G \vdash A\oplus B,\D$}
	\DP
}
\def\LLplusRdeux{
	\AXC	{$\G \vdash B,\D$}
	\RightLabel{$(\oplus R_{2})$}
	\UIC{$\G \vdash A\oplus B,\D$}
	\DP
}
\def\LLparalleleL{
	\AXC	{$\G,A \vdash \D$}
	\AXC	{$\G',B \vdash \D'$}
	\RightLabel{$(\revAnd L)$}
	\BIC{$\G,\G',A\revAnd B \vdash \D,\D'$}
	\DP
}
\def\LLparalleleR{
	\AXC	{$\G \vdash A,B,\D$}
	\RightLabel{$(\revAnd R)$}
	\UIC{$\G \vdash A\revAnd B,\D$}
	\DP
}
\def\LLimpL{
	\AXC	{$\G \vdash A,\D$}
	\AXC	{$\G',B \vdash \D'$}
	\RightLabel{$(\multimap\! L)$}
	\BIC{$\G,\G',A\multimap B \vdash \D,\D'$}
	\DP
}
\def\LLimpR{
	\AXC	{$\G,A \vdash B,\D$}
	\RightLabel{$(\multimap\! R)$}
	\UIC{$\G \vdash A\multimap B,\D$}
	\DP
}


%LL, constantes
\def\LLunL{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(\1 L)$}
	\UIC{$\G,\1 \vdash \D$}
	\DP
}
\def\LLunR{
	\AXC	{}
	\RightLabel{$(\1 R)$}
	\UIC{$\; \vdash \1$}
	\DP
}
\def\LLzeroL{
	\AXC	{}
	\RightLabel{$(\0 L)$}
	\UIC{$\0 \vdash \;$}
	\DP
}
\def\LLzeroR{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(\0 R)$}
	\UIC{$\G \vdash \0,\D$}
	\DP
}
\def\LLbotL{
	\AXC	{}
	\RightLabel{$(\bot L)$}
	\UIC{$\G,\bot \vdash \D$}
	\DP
}
\def\LLtopR{
	\AXC	{}
	\RightLabel{$(\top R)$}
	\UIC{$\G \vdash \top,\D$}
	\DP
}

%LL, modalités
\def\LLbangL{
	\AXC	{$\G,A \vdash \D$}
	\RightLabel{$(! L)$}
	\UIC{$\G,!A \vdash \D$}
	\DP
}
\def\LLbangR{
	\AXC	{$!\G \vdash A,?\D$}
	\RightLabel{$(! R)$}
	\UIC{$!\G \vdash !A,!\D$}
	\DP
}
\def\LLwhynotL{
	\AXC	{$!\G,A \vdash ?\D$}
	\RightLabel{$(? L)$}
	\UIC{$!\G,?A \vdash ?\D$}
	\DP
}
\def\LLwhynotR{
	\AXC	{$\G \vdash A,\D$}
	\RightLabel{$(? R)$}
	\UIC{$\G \vdash ?A,\D$}
	\DP
}

%LL, règles structurelles
\def\LLweakeningL{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(weakening\ L)$}
	\UIC{$\G,!A \vdash \D$}
	\DP
}
\def\LLweakeningR{
	\AXC	{$\G \vdash \D$}
	\RightLabel{$(weakening\ R)$}
	\UIC{$\G \vdash ?A,\D$}
	\DP
}
\def\LLcontractionL{
	\AXC	{$\G,!A,!A \vdash \D$}
	\RightLabel{$(contraction\ L)$}
	\UIC{$\G,!A \vdash \D$}
	\DP
}
\def\LLcontractionR{
	\AXC	{$\G \vdash ?A,?A,\D$}
	\RightLabel{$(contraction\ R)$}
	\UIC{$\G \vdash ?A,\D$}
	\DP
}
\def\LLexchangeL{
	\AXC	{$\G_{1},A,B,\G_{2} \vdash \D$}
	\RightLabel{$(exchange\ L)$}
	\UIC{$\G_{1},B,A,\G_{2} \vdash \D$}
	\DP
}
\def\LLexchangeR{
	\AXC	{$\G \vdash \D_{1},A,B,\D_{2}$}
	\RightLabel{$(exchange\ R)$}
	\UIC{$\G \vdash \D_{1},B,A,\D_{2}$}
	\DP
}





\begin{figure}[h]
\centering
{
\renewcommand{\arraystretch}{2}
\resizebox{0.8\linewidth}{!}{
$
\begin{tabular}{|cc|}
 	\hline
	\multicolumn{1}{|c|}{\id} & \cut \\ \hline
	\LLnonL & \LLnonR \\ %\hline
	\LLtenseurL & \LLtenseurR \\ %\hline
	\LLavecLun \; \LLavecLdeux & \LLavecR \\ %\hline
	\LLplusL & \LLplusRun \; \LLplusRdeux \\ %\hline
	\LLparalleleL & \LLparalleleR \\ %\hline
	\LLimpL & \LLimpR \\ %\hline
	\LLunL & \LLunR \\
	\LLzeroL & \LLzeroR \\
%	\LLbotL & \LLtopR \\
%\hspace{-2.5mm}
%	\LLunL \; \LLzeroL & \LLunR \; \LLzeroR \\ \hline
	\LLbotL & \LLtopR \\ \hline
	\LLbangL & \LLbangR \\
	\LLwhynotL & \LLwhynotR \\ \hline
%	\LLbangL \; \LLwhynotL & \LLbangR  \; \LLwhynotR \\ \hline
	\LLweakeningL & \LLweakeningR \\ %\hline
	\LLcontractionL & \LLcontractionR \\ %\hline
	\LLexchangeL & \LLexchangeR \\ \hline
\end{tabular}
$
}
}
\caption{Les règles d'un calcul des séquents pour LL}
\label{fig:reglesLL}
\end{figure}

\noindent
\textit{Remarque.}
Une dualité importante liée au connecteur $\lnot$ permet de définir un calcul des séquents pour la logique linéaire dans lequel les séquents ne comportent aucune formule à gauche. On a choisi de conserver des séquents avec des formules des deux côtés par similarité avec les autres calculs de séquents étudiés.


\subsection{Limitation des règles structurelles, nouveaux connecteurs, modalités}
\label{subsection:connecteursetc}

L'usage des règles structurelles est restreint à certains types de formules qu'on verra plus loin. Une conséquence importante est qu'on n'a plus l'équivalence entre les deux écritures possibles des règles liées à $\land$, vues en \ref{subsection:VariantesEcritureRegles}. C'est pourquoi on introduit deux connecteurs distincts $\otimes$ et $\&$, qui pourraient tous deux se lire ``et'' en première approximation, correspondant chacun à une définition possible de $\land$ dans \LK\ (cf. les règles liées à ces connecteurs dans la figure~\ref{fig:reglesLL}). De même, le $\lor$ de logique classique est séparé en deux connecteurs $\oplus$ et $\revAnd$. On souhaite tout de même garder des règles structurelles, dont l'usage est seulement limité. On introduit pour cela des connecteurs unaires \emph{modaux}, ou \emph{modalités}, $!$ et $?$. Les règles structurelles peuvent seulement être appliquées à des formules comportant un de ces connecteurs : $!$ si la formule est à gauche, $?$ si elle est à droite. On a aussi des règles d'introduction de ces connecteurs, où par exemple $!\G$ représente une liste dont toutes les formules sont de la forme $!A$. On retrouve des relations entre $\otimes$ et $\&$ grâce à ces modalités : par exemple $(!A)\otimes(!B) =\ !(A\& B)$, qu'on discute dans la prochaine sous-section.



\subsection{Interprétation : une idée de ``ressources''}
\label{subsection:ressources}

La logique linéaire est une logique de ``ressources''. L'implication linéaire $A\multimap B$ peut en effet se comprendre comme ``on peut dépenser un objet de type $A$ pour obtenir un objet de type $B$''. Ici, ``type'' est simplement un mot du langage courant et non un terme mathématique ou informatique. % ; on aurait aussi bien pu dire ``un objet de sorte $A$'', mais cela  
La formule $A\otimes B$ représente la possession à la fois d'un objet de type $A$ et d'un autre objet de type $B$. On comprend alors la notion de ``dépense'' en remarquant que, si on a $A\multimap B$ et $A\multimap C$, on n'obtient pas pour autant $A\multimap B\otimes C$ car un seul objet de type $A$ ne peut pas être dépensé deux fois ; en revanche on obtient bien $A\otimes A \multimap B\otimes C$. On n'obtient pas non plus $A\otimes A\otimes A \multimap B\otimes C$ car un objet de type $A$ ne serait pas dépensé ; cela est lié au fait que la formule $A\multimap\1$ n'est pas universellement valide, où $\1$ est l'élément neutre pour $\otimes$. Une analogie courante et pertinente compare ce fragment de la logique linéaire aux équations de réaction en chimie, avec la maxime de Lavoisier bien connue ``Rien ne se perd, rien ne se crée, tout se transforme''. Par exemple, l'équation $CH_{4} + 2O_{2} \longrightarrow CO_{2} + 2H_{2}O$ peut être fidèlement représentée par la formule $!(CH_{4}\otimes O_{2}\otimes O_{2} \multimap CO_{2}\otimes H_{2}O\otimes H_{2}O)$, où $CH_{4}$ etc. sont considérés comme des constantes ; la modalité $!$, discutée plus loin, signifie qu'on peut appliquer cette réaction autant de fois qu'on le souhaite.

Intéressons-nous maintenant à $\&$, l'autre connecteur issu du $\land$ de la logique classique. La formule $A\& B$ représente la %possession
possibilité de posséder%
, au choix, un objet de type $A$ ou un objet de type $B$. \`A partir de $A\multimap B$ et $A\multimap C$, on obtient bien $A\multimap B\&C$ : on a le choix de la fa\c con dont on dépense l'objet de type $A$. Le fait qu'on ne possède finalement qu'un seul objet, de type $A$ ou de type $B$, peut donner l'impression qu'il s'agit d'une disjonction. Le point important est la possibilité de choisir %soi-même 
le type parmi $A$ et $B$. Cela entraîne qu'on peut prouver $A\&B\multimap A$ et $A\&B\multimap B$, mais pas $A\multimap A\&B$, donc il s'agit bien d'une conjonction. C'est $A\oplus B$ qui représente la possession d'un objet dont on sait que le type est $A$ ou $B$, 
%sans possibilité de choisir
mais on ne peut pas choisir lequel des deux%
. Il s'agit cette fois d'une disjonction : on peut prouver $A\multimap A\oplus B$ mais pas $A\oplus B\multimap A$.

La modalité $!A$ signifie qu'on peut posséder un objet de type $A$ autant de fois qu'on le souhaite, y compris zéro. Un exemple significatif est l'égalité $(!A)\otimes(!B) =\ !(A\& B)$ : posséder simultanément autant d'objets qu'on veut de type $A$ et autant d'objets qu'on veut de type $B$, revient à avoir autant de fois qu'on veut la possibilité de choisir de posséder un objet de type $A$ ou un objet de type $B$.

Les connecteurs $\revAnd$ et $?$ sont plus difficiles à expliquer naturellement.


\subsection{Logique linéaire intuitionniste}

La logique linéaire intuitionniste (ILL) peut être associée à un calcul dont les séquents sont les mêmes que ceux de \LJ\ : une liste de formules $\G$ et une formule $D$, avec la notation $\G \vdash D$ ; les règles, empruntées pour la plupart à (\cite{theseDLW}, p.75), sont données dans la figure~\ref{fig:reglesILL}.

La logique linéaire intuitionniste (ILL) est obtenue à partir de la logique linéaire de la même manière que la logique intuitionniste est obtenue à partir de la logique classique : en se restreignant aux séquents avec une unique formule à droite. 
On enlève les connecteurs et la constante qui sont liés à des règles qui ne s'adaptent pas aux séquents avec une unique formule à droite, à savoir $\lnot$, $\revAnd$, $\0$ et $?$.

%On ne garde que les connecteurs binaires $\otimes$, $\multimap$ et $\&$ avec leur élément neutre éventuel, et le connecteur modal $!$.
%En effet, les autres connecteurs sont liés à des règles qui ne s'adaptent pas aux séquents avec une unique formule à droite, à l'exception de $\oplus$ qui pourrait être conservé sans problème. Néanmoins, on se conforme à une tradition qui consiste à parler de \emph{logique linéaire intuitionniste} lorsqu'on ne considère pas de connecteur $\oplus$, et de \emph{logique linéaire intuitionniste à produits finis} lorsqu'on le rajoute.
%Le connecteur de logique linéaire $\revAnd$ disparaît alors, puisque les règles liées nécessitent des séquents avec plusieurs formules à droite. Le connecteur $\oplus$ peut être conservé, mais traditionnellement, il est absent de ce qu'on appelle la \emph{logique linéaire intuitionniste}, qui devient la \emph{logique linéaire intuitionniste à produits finis} lorsqu'on le rajoute.



%ILL
%id, coupure : commun

%ILL,règles logiques
\def\ILLtenseurL{
	\AXC	{$\G,A,B \vdash D$}
	\RightLabel{$(\otimes L)$}
	\UIC{$\G,A\otimes B \vdash D$}
	\DP
}
\def\ILLtenseurR{
	\AXC	{$\G \vdash A$}
	\AXC	{$\G' \vdash B$}
	\RightLabel{$(\otimes R)$}
	\BIC{$\G \vdash A\otimes B$}
	\DP
}
\def\ILLavecLun{
	\AXC	{$\G,A \vdash D$}
	\RightLabel{$(\& L_{1})$}
	\UIC{$\G,A\& B \vdash D$}
	\DP
}
\def\ILLavecLdeux{
	\AXC	{$\G,B \vdash D$}
	\RightLabel{$(\& L_{2})$}
	\UIC{$\G,A\& B \vdash D$}
	\DP
}
\def\ILLavecR{
	\AXC	{$\G \vdash A$}
	\AXC	{$\G \vdash B$}
	\RightLabel{$(\& R)$}
	\BIC{$\G \vdash A\& B$}
	\DP
}
\def\ILLplusL{
	\AXC	{$\G,A \vdash D$}
	\AXC	{$\G,B \vdash D$}
	\RightLabel{$(\oplus L)$}
	\BIC{$\G,A\oplus B \vdash D$}
	\DP
}
\def\ILLplusRun{
	\AXC	{$\G \vdash A$}
	\RightLabel{$(\oplus R_{1})$}
	\UIC{$\G \vdash A\oplus B$}
	\DP
}
\def\ILLplusRdeux{
	\AXC	{$\G \vdash B$}
	\RightLabel{$(\oplus R_{2})$}
	\UIC{$\G \vdash A\oplus B$}
	\DP
}
\def\ILLimpL{
	\AXC	{$\G \vdash A$}
	\AXC	{$\G',B \vdash D$}
	\RightLabel{$(\multimap\! L)$}
	\BIC{$\G,\G',A\multimap B \vdash D$}
	\DP
}
\def\ILLimpR{
	\AXC	{$\G,A \vdash B$}
	\RightLabel{$(\multimap\! R)$}
	\UIC{$\G \vdash A\multimap B$}
	\DP
}


%ILL, constantes
\def\ILLunL{
	\AXC	{$\G \vdash D$}
	\RightLabel{$(\1 L)$}
	\UIC{$\G,\1 \vdash D$}
	\DP
}
\def\ILLunR{
	\AXC	{}
	\RightLabel{$(\1 R)$}
	\UIC{$\; \vdash \1$}
	\DP
}
\def\ILLbotL{
	\AXC	{}
	\RightLabel{$(\bot L)$}
	\UIC{$\G,\bot \vdash D$}
	\DP
}
\def\ILLtopR{
	\AXC	{}
	\RightLabel{$(\top R)$}
	\UIC{$\G \vdash \top,D$}
	\DP
}

%ILL, modalités
\def\ILLbangL{
	\AXC	{$\G,A \vdash D$}
	\RightLabel{$(! L)$}
	\UIC{$\G,!A \vdash D$}
	\DP
}
\def\ILLbangR{
	\AXC	{$!\G \vdash A$}
	\RightLabel{$(! R)$}
	\UIC{$!\G \vdash !A$}
	\DP
}

%ILL, règles structurelles
\def\ILLweakeningL{
	\AXC	{$\G \vdash D$}
	\RightLabel{$(weakening\ L)$}
	\UIC{$\G,!A \vdash D$}
	\DP
}
\def\ILLcontractionL{
	\AXC	{$\G,!A,!A \vdash D$}
	\RightLabel{$(contraction\ L)$}
	\UIC{$\G,!A \vdash D$}
	\DP
}
\def\ILLexchangeL{
	\AXC	{$\G_{1},A,B,\G_{2} \vdash D$}
	\RightLabel{$(exchange\ L)$}
	\UIC{$\G_{1},B,A,\G_{2} \vdash D$}
	\DP
}




\begin{figure}[h]
\centering
{
\renewcommand{\arraystretch}{2}
\resizebox{0.8\linewidth}{!}{
$
\begin{tabular}{|cc|}
 	\hline
	\multicolumn{1}{|c|}{\id} & \LJcut \\ \hline
	\ILLtenseurL & \ILLtenseurR \\ %\hline
	\ILLavecLun \; \ILLavecLdeux & \ILLavecR \\ %\hline
	\ILLplusL & \ILLplusRun \; \ILLplusRdeux \\
	\ILLimpL & \ILLimpR \\ %\hline
	\ILLunL & \ILLunR \\
	\ILLbotL & \ILLtopR \\ \hline
	\ILLbangL & \ILLbangR \\ \hline
	\multicolumn{2}{|c|}{\ILLweakeningL \; \ILLcontractionL \; \ILLexchangeL} \\ \hline\end{tabular}
$
}
}
\caption{Les règles d'un calcul des séquents pour ILL}
\label{fig:reglesILL}
\end{figure}



\section{Le procédé d'élimination de la coupure (p.é.c.) pour ILL}
\label{section:pec}

\subsection{Le théorème d'élimination de la coupure}

La règle de coupure :\cut pour des séquents classiques,\\\LJcut pour des séquents intuitionnistes, est, tout comme\id, une règle qu'on retrouve dans la plupart des calculs de séquents. Elle est naturelle lorsqu'on considère l'interprétation d'un séquent classique $\G\vdash\D$ : ``à partir des formules de $\G$, on peut montrer une formule de $\D$''. La règle de coupure signifie alors que si on a $\G\vdash A,\D$ et $\G',A\vdash\D'$ alors, ou bien à partir de $\G$, on peut monter une formule de $\D$, auquel cas on a aussi $\G,\G'\vdash\D,\D'$, ou bien à partir de $\G$ on peut montrer 
$A$, or à partir de $A$ et $\G'$ on peut montrer une formule de $\D'$, donc à partir de $\G,\G'$ on peut montrer une formule de $\D'$, si bien qu'on a encore $\G,\G'\vdash\D,\D'$. Cette interprétation s'adapte aussi aux séquents intuitionnistes.
L'idée principale est qu'une conclusion établie peut être immédiatement utilisée comme hypothèse (en rappelant que dans $\G\vdash\D$ les formules de $\G$ sont appelées les hypothèses et celles de $\D$ les conclusions). La règle de coupure représente une transitivité : de $A\vdash B$ et $B\vdash C$ on déduit $A\vdash C$.

Le \emph{théorème d'élimination de la coupure}, pour un calcul donné présentant une règle de coupure, affirme que si on supprime la règle de coupure du calcul, on obtient un calcul équivalent, c'est-à-dire que les séquents prouvables restent les mêmes. Une formulation équivalente est : tout séquent prouvable dans le calcul considéré admet un arbre de preuve dans lequel la règle de coupure ne figure pas. C'est cette formulation qui est utilisée dans la démonstration que nous allons étudier. Le théorème d'élimination de la coupure est vérifié par de nombreux calculs de séquents, notamment tous ceux que nous avons introduits dans ce mémoire. Il est intéressant d'un point de vue sémantique : la transitivité représentée par la règle de coupure est vérifiée par le calcul, même si on ne l'impose pas par un énoncé explicite de cette règle de coupure. Souvent valide, ce théorème est néanmoins fréquemment difficile à établir.
%Le théorème d'élimination de la coupure est souvent valide, mais aussi souvent difficile à établir. 


\subsection{\'Eléments d'une démonstration de ce théorème pour ILL}

\noindent
\textit{Notation.}
Soit $p$ une preuve du séquent $\s$. On écrit%
	\AXC	{$p$}
	%\noLine
	\dottedLine
	\UIC{$\;\s\;$}
	\DP
car souvent, il est intéressant d'expliciter $\s$ pour prolonger $p$ en une preuve plus complexe d'un autre séquent.

\vspace{.5em}

Nous présentons un bref aper\c cu d'une démonstration constructive du théorème d'élimination de la coupure dans le cadre du calcul des séquents lié à la logique linéaire intuitionniste que nous avons présenté. Elle figure dans le chapitre~3 de~\cite{panorama}. En donner les détails serait bien trop long : dans le livre cité, elle occupe une vingtaine de pages. Elle fournit une liste précise de tranformations élémentaires, qui à une preuve d'un séquent associent une autre preuve du même séquent. L'idée est qu'en les appliquant successivement dans un ordre bien choisi à une preuve d'un séquent, on peut obtenir une preuve du même séquent n'utilisant jamais la règle de coupure. Un premier point à noter est qu'une transformation élémentaire peut être appliquée à un sous-arbre d'une preuve donnée : par exemple, une preuve
\resizebox{1.5cm}{!}{
	\AXC	{$p$}
	\dottedLine
	\UIC{$\;\s\;$}
	\RightLabel{$(\mathcal R)$}
	\UIC{$\;\s'\;$}
	\DP
}
peut être transformée en 
\resizebox{1.5cm}{!}{
	\AXC	{$p'$}
	\dottedLine
	\UIC{$\;\s\;$}
	\RightLabel{$(\mathcal R)$}
	\UIC{$\;\s'\;$}
	\DP
}
si la preuve $p$ peut être transformée en $p'$. Les transformations élémentaires sont locales : le nombre de n\oe uds explicités lorsqu'on en énonce une ne dépasse pas une dizaine.
%
Bien que la coupure ne soit pas une règle logique, on appelle \emph{formule principale} de\LJcut la formule $A$ qui a un rôle particulier.
L'idée associée aux applications successives de transformations élémentaires est de ``faire remonter'' les utilisations de la règle de coupure à une profondeur de plus en plus grande dans l'arbre.
Pour cela, on a parfois besoin de remplacer des utilisations de cette règle par d'autres dont la formule principale est plus petite, quitte à augmenter la taille de l'arbre.
%, et d'autre part, de remplacer des utilisations de cette règle par d'autres dont la formule principale est plus petite. %L'intérêt de diminuer la taille de la formule principale est de finalement arriver à des formules atomiques, qui 
Il faut bien entendu que certaines transformations permettent de réduire strictement le nombre d'utilisations de la règle de coupure : par exemple,
\\
\vspace{-.8em}

{\centering
\resizebox{5cm}{!}{
	\AXC	{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
		\AXC	{$p$}
		\dottedLine
		\UIC{$\G_{1},A,\G_{2}\vdash B$}
	\RightLabel{$(cut)$}
	\BIC{$\G_{1},A,\G_{2}\vdash B$}
	\DP
}
\quad se transforme en\quad
	\AXC	{$p$}
	\dottedLine
	\UIC{$\G_{1},A,\G_{2}\vdash B$}
	\DP
.

}
\vspace{.2em}

\noindent
Un exemple de transformation permettant de diminuer la taille de la formule principale est celle, associée au connecteur $\otimes$, qui transforme
\\
\vspace{-.8em}

{\centering
	\AXC	{}
	\RightLabel{$(id)$}
	\UIC{$A\otimes B\vdash A\otimes B$}
	\DP
\quad en \quad
\resizebox{4.5cm}{!}{
	\AXC	{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
		\AXC	{}
		\RightLabel{$(id)$}
		\UIC{$B\vdash B$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A,B\vdash A\otimes B$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes B\vdash A\otimes B$}
	\DP
}
.

}
\noindent
Le choix de la transformation à appliquer selon la situation est très compliqué : il dépend notamment de la nature des règles qui se situent au-dessus de la règle de coupure qu'on souhaite ``faire remonter'' (éventuellement en plusieurs transformations), mais ce choix n'est parfois pas le même selon que la formule principale d'une de ces règles est ou non égale à la formule principale de la coupure considérée...

\vspace{.5em}

On admettra que le procédé qui consiste à appliquer ces transformations dans un ordre bien choisi fonctionne. On renvoie au chapitre~3 de~\cite{panorama} pour le détail des transformations et du choix de la transformation à appliquer selon la situation.



\subsection{Procédé d'élimination de la coupure et relation d'équivalence sur les preuves}

On appelle \emph{procédé d'élimination de la coupure}, abrégé en p.é.c., le procédé décrit dans la preuve précédente. Celui-ci est caractérisé par l'ensemble de transformations dont il était question dans la preuve, mais aussi par un choix déterministe de transformation à appliquer selon la situation. On peut construire à partir de ce p.é.c. une relation d'équivalence sur les preuves de logique linéaire intuitionniste. Pour cela, on pose $p\rhd p'$ si on peut passer de la preuve $p$ à la preuve $p'$ en appliquant le p.é.c., puis on considère la clôture symétrique et transitive de $\rhd$. On appelle \emph{équivalence selon le p.é.c.} la relation d'équivalence ainsi obtenue. On remarque que si deux preuves sont équivalentes selon le p.é.c., alors il s'agit de preuves du même séquent.



% on impose que si on peut passer d'une preuve $p$ à une preuve $p'$ en appliquant le p.é.c., alors $p$ et $p'$ sont équivalentes, puis on ajoute les 


% : deux preuves $p$ et $p'$ sont équivalentes si on peut passer de l'une à l'autre en appliquant le p.é.c. (il s'agit alors nécessairement de preuves du même séquent). On parle d'\emph{équivalence selon le p.é.c.}\,. La transitivité est assurée par le déterminisme du procédé : par exemple si le p.é.c. transforme une preuve $p$ en $p'$ et transforme cette même preuve $p$ en $p''$, en itérant l'application de la transformation convenablement choisie à partir de $p$, on obtient un jour $p'$ et on obtient un jour $p''$, et celle des deux qui est rencontrée la première est transformée en l'autre par le p.é.c.\,.

Dans le chapitre~3 de~\cite{panorama}, le p.é.c. est défini très soigneusement, en ne laissant pas plus de liberté que celle qui est nécessaire pour éliminer effectivement toutes les utilisations de la règle de coupure. On qualifiera ce p.é.c. de ``strict''. Il existe en effet un p.é.c. ``large'', obtenu à partir du précédent en autorisant, dans certaines situations précises, l'application d'une transformation différente de celle indiquée par le p.é.c. ``strict''. On verra que 
%donner cette liberté supplémentaire 
considérer la relation d'équivalence associée %de la même manière au p.é.c. ``large''
peut permettre d'obtenir des propriétés intéressantes sur la catégorie qu'on contruira dans la partie suivante à partir des preuves de la logique linéaire intuitionniste.


\section{Construction d'une catégorie et propriétés de cette catégorie}

%\idees{
%On suppose qu'on a présenté la logique linéaire et son calcul des séquents, et le procédé d'élimination de la coupure (qu'on abrège pour l'instant parfois en p.é.c., mais cela peut changer). On distingue le p.é.c. ``strict'' avec seulement les transformations nécessaires pour le théorème d'élimination de la coupure, du p.é.c. ``large'' où on a rajouté les transformations qui font que alpha, lambda, rho deviennent des isomorphismes et donc permettent d'obtenir vraiment une catégorie monoïdale.
%}
%
%\idees{source : chapitre 3}

Cette section est fortement inspirée du chapitre~2 de~\cite{panorama}. On renvoie à celui-ci pour les démonstrations, qui s'appuient souvent sur certaines transformations du procédé d'élimination de la coupure figurant dans le chapitre~3 du même ouvrage et qu'on ne peut pas envisager de recopier ici intégralement.

\subsection{Invariant modulaire et catégorie}


\`A chaque preuve $p$, on associe une \textbf{\emph{dénotation}} $[p]$. On veut que cela constitue un invariant selon l'élimination de la coupure : deux preuves ont la même dénotation si, et seulement si, %une peut être obtenue en appliquant à l'autre des transformations autorisées par la procédure d'élimination de la coupure.
elles sont équivalentes selon le procédé d'élimination de la coupure (cf. section précédente).
Ceci est motivé par une analogie avec la théorie des n\oe uds, où les invariants sont relatifs aux transformations de Reidemeister.

\begin{floatingfigure}[r]{0.27\textwidth}
\centering
	\AXC	{$p_{1}$}
	\dottedLine
	\UIC{$A\vdash B$}
	\AXC	{$p_{2}$}
	\dottedLine
	\UIC{$B\vdash C$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash C$}
	\noLine
	\UIC{}
	\noLine
	\UIC{$p$}
	\DP\\
\end{floatingfigure}


On demande également la propriété suivante. Soit $A$, $B$, $C$ des formules, et $p_{1}$ et $p_{2}$ des preuves de $A \vdash B$ et $B \vdash C$ respectivement. La règle de coupure fournit immédiatement la preuve $p$ de $A \vdash C$ ci-contre. On veut que sa dénotation $[p]$ se déduise à partir de $[p_{1}]$ et $[p_{2}]$. On introduit pour cela la \textbf{loi de composition $\circ$} telle que $[p] = [p_{2}] \circ [p_{1}]$.
%On dit alors que l'invariant est \emph{modulaire}.

On remarque que la loi de composition $\circ$ est \textbf{associative} et présente pour chaque formule une \textbf{identité} à gauche et à droite. En effet, soit $A$, $B$, $C$, $D$ des formules, et $p_{1}$, $p_{2}$, $p_{3}$ des preuves respectives de $A \vdash B$, $B \vdash C$, $C \vdash D$. On peut en déduire deux preuves de $A \vdash D$ :
\\
\vspace{-1.5em}
\begin{center}
	\AXC	{$p_{1}$}
	\dottedLine
	\UIC{$A\vdash B$}
	\AXC	{$p_{2}$}
	\dottedLine
	\UIC{$B\vdash C$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash C$}
	\AXC	{$p_{3}$}
	\dottedLine
	\UIC{$C\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash D$}
	\DP
\quad et \quad
	\AXC	{$p_{1}$}
	\dottedLine
	\UIC{$A\vdash B$}
	\AXC	{$p_{2}$}
	\dottedLine
	\UIC{$B\vdash C$}
	\AXC	{$p_{3}$}
	\dottedLine
	\UIC{$C\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$B\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash D$}
	\DP
\end{center}

\noindent
qui sont équivalentes selon le p.é.c.\,. Cela signifie précisément que $[p_{3}]\circ([p_{2}]\circ[p_{1}]) = ([p_{3}]\circ[p_{2}])\circ[p_{1}]$. L'identité pour une formule $A$ est la dénotation de la preuve 
{\centerAlignProof
	\AXC	{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	\DP
}
; on note cette dénotation $id_{A}$. Soit $p$ une preuve de $A \vdash B$, les deux preuves 
%\\
%%\def\proofSkipAmount{\vskip -10em}
%\begin{center}
	\AXC	{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	\AXC	{$p$}
	\dottedLine
	\UIC{$A\vdash B$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash B$}
	\DP
%\qquad\qquad et \qquad\qquad
et
	\AXC	{$p$}
	\dottedLine
	\UIC{$A\vdash B$}
	\DP
%\end{center}
%
%\noindent
sont équivalentes selon l'élimination de la coupure, ce qui signifie que $[p]\circ id_{A}=[p]$. De même, pour $p$ une preuve de $B\vdash A$, on a $id_{A}\circ[p]=[p]$.

Ces propriétés sur les dénotations permettent d'utiliser le formalisme des catégories.

% def catégorie
\begin{df}
Une \textbf{\emph{catégorie}} consiste en une collection d'\emph{objets} et une collection de \emph{morphismes}, cette dernière munie d'une opération binaire partielle $\circ$ appelée \emph{composition}, avec les propriétés suivantes.

\begin{itemize}

\item
\`A chaque morphisme $f$ est associé un couple d'objets $(A,B)$ ; on écrit $f:A\to B$.
On dit que $A\to B$ est le \emph{type} de $f$, et que $f$ est un morphisme \emph{de} $A$ \emph{vers} $B$, et encore que $A$ est le \emph{domaine} ou la \emph{source} de $f$, et $B$ le \emph{codomaine} ou la \emph{cible} de $f$.

\item
Pour tous objets $A$, $B$, $C$ et morphismes $f$, $g$ tels que $f:A\to B$ et $g:B\to C$, le morphisme $g\circ f$ existe et $g\circ f:A\to C$.

\item
\textbf{Identité.}
Pour tout objet $A$, il existe un morphisme particulier $id_{A}:A\to A$ appelé \emph{identité sur $A$}, tel que 
%pour tous objet $B$ et morphismes $f:B\to A$ et $g:A\to B$, $id_{A}\circ f=f$ et $g\circ id_{A}=g$.
pour tout objet $B$, pour tout $f:B\to A$, $id_{A}\circ f=f$ et pour tout $g:A\to B$, $g\circ id_{A}=g$.

\item
\textbf{Associativité.}
Pour tous $f:A\to B$, $g:B\to C$, $h:C\to D$, on a $h\circ(g\circ f) = (h\circ g)\circ f$.
\end{itemize}

\

\noindent\textbf{Notations.} Soit $\Ccal$ une catégorie, on note $Obj(\Ccal)$ la classe de ses objets et $Hom(\Ccal)$ celle de ses morphismes. Pour des objets $A$ et $B$, on note $Hom_{A\to B}(\Ccal)$ la classe des morphismes de type $A\to B$. On peut omettre l'argument $\Ccal$ s'il n'y a pas d'ambiguïté, par exemple si on travaille sur une seule catégorie.
\end{df}

%Les dénotations des preuves s'organisent sous la forme de
On construit la catégorie suivante, qu'on appellera $\CProofs$.
%\idees{choix du nom ?}
\`A chaque formule $A$ on associe une dénotation $[A]$. Les dénotations des formules %, deux à deux distinctes, 
constituent les objets de la catégorie. Les morphismes sont des dénotations de preuves. Les morphismes de $[A]$ vers $[B]$ sont les dénotations des preuves de $A\vdash B$ ; si ce séquent n'est pas prouvable, il n'y en a pas. Comme on l'a vu, la composition $\circ$ est bien associative, et l'identité sur $[A]$ est le morphisme $id_{A}$. On notera aussi $A$ la dénotation de la formule $A$, sauf lorsqu'on souhaite insister sur le fait qu'il s'agit d'une dénotation. Ceci permet d'alléger l'écriture et de retrouver la notation employée dans les définitions sur les catégories.

%Pour l'instant, on ne considère 
Cette définition ne tient compte que des dénotations de preuves où le séquent prouvé a une unique formule à gauche et une unique formule à droite ; les autres séquents des preuves peuvent avoir n'importe quelle forme. On donne rapidement, à la fin de cette section, quelques éléments qui permettraient d'étendre ce formalisme à des preuves de n'importe quel séquent intuitionniste.



%Cette restriction est conservée dans 
%\idees{quelques sous-sections ? tout le reste de la partie ? toujours ? Est-ce qu'on peut étendre ce formalisme à n'importe quelle preuve de la logique intuitionniste linéaire grâce au théorème de MacLane, cf. mail ?}
%les quelques sous-section qui suivent, où on enrichit notre catégorie. ... Enfin, on étend ce formalisme à toutes les preuves de la logique linéaire intuitionniste, %c'est-à-dire avec des séquents qui 
%où les séquents ont un nombre quelconque de formules à gauche et exactement une formule à droite.

%


\subsection{Produit tensoriel et bifoncteur}

\def\sizedefotimes{0.27\textwidth}
\begin{floatingfigure}[r]{\sizedefotimes}
\centering
\resizebox{\sizedefotimes}{!}{
	\AXC	{$p_{1}$}
	\dottedLine
	\UIC{$A_{1}\vdash B_{1}$}
	   \AXC{$p_{2}$}
	   \dottedLine
	   \UIC{$A_{2}\vdash B_{2}$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A_{1},A_{2} \vdash B_{1}\otimes B_{2}$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A_{1}\otimes A_{2} \vdash B_{1}\otimes B_{2}$}
	\noLine
	\UIC{}
	\noLine
	\UIC{$p$}
\DP
}
\\
\end{floatingfigure}




%Le connecteur de logique linéaire $\otimes$ permet de définir un produit tensoriel sur les dénotations de formules, aussi noté $\otimes$ : on pose $[A]\otimes[B]=[A\otimes B]$.
Le connecteur de logique linéaire $\otimes$, appelé \emph{produit tensoriel}, induit un opérateur sur les dénotations de formules, aussi noté $\otimes$ : on pose $[A]\otimes[B]=[A\otimes B]$.
On souhaite étendre cet opérateur aux dénotations de preuves. Pour cela, on remarque qu'à partir de preuves $p_{1}$ de $A_{1}\vdash B_{1}$ et $p_{2}$ de $A_{2}\vdash B_{2}$
%	\AXC	{$p_{1}$}
%	\dottedLine
%	\UIC{$A_{1}\vdash B_{1}$}
%	\DP
%et
%	\AXC	{$p_{2}$}
%	\dottedLine
%	\UIC{$A_{2}\vdash B_{2}$}
%	\DP
, on peut déduire la preuve $p$ ci-contre de $A_{1}\otimes A_{2} \vdash B_{1}\otimes B_{2}$. On définit alors ${[p_{1}]\otimes[p_{2}]=[p]}$.

L'opérateur est bien défini : si on a deux autres preuves $p'_{1}$ et $p'_{2}$ telles que $[p'_{1}]=[p_{1}]$ et $[p'_{2}]=[p_{2}]$, et si $p'$ est la preuve obtenue à partir de $p'_{1}$ et $p'_{2}$ selon le procédé utilisé pour construire $p$, alors $[p']=[p]$. En effet, le procédé d'élimination de la coupure autorise évidemment à remplacer la preuve $p_{1}$ apparaissant dans la preuve $p$ par une preuve $p'_{1}$ qui lui est équivalente.

Intéressons-nous à la compatibilité de $\otimes$ avec $\circ$. Soit des morphismes  $f_{1} : A_{1}\to B_{1}$\,,\; $f_{2} : A_{2}\to B_{2}$\,,\; $g_{1} : B_{1}\to C_{1}$\,,\; $g_{2} : B_{2}\to C_{2}$ %(on confond un objet avec sa dénotation pour simplifier l'écriture)
. Pour chaque morphisme $f$, soit $p(f)$ une preuve de dénotation $f$. Les deux preuves suivantes sont équivalentes selon le p.é.c., ce qui signifie que $(g_{1}\otimes g_{2})\circ(f_{1}\otimes f_{2}) = (g_{1}\circ f_{1})\otimes(g_{2}\circ f_{2})$.
\vspace{2mm}

\noindent
\resizebox{\linewidth}{!}{
	\AXC	{$p(f_{1})$}
	\dottedLine
	\UIC{$A_{1}\vdash B_{1}$}
		\AXC{$p(f_{2})$}
		\dottedLine
		\UIC{$A_{2}\vdash B_{2}$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A_{1},A_{2} \vdash B_{1}\otimes B_{2}$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A_{1}\otimes A_{2} \vdash B_{1}\otimes B_{2}$}
		\AXC	{$p(g_{1})$}
		\dottedLine
		\UIC{$B_{1}\vdash C_{1}$}
			\AXC{$p(g_{2})$}
			\dottedLine
			\UIC{$B_{2}\vdash C_{2}$}
		\RightLabel{$(\otimes R)$}
		\BIC{$B_{1},B_{2} \vdash C_{1}\otimes C_{2}$}
		\RightLabel{$(\otimes L)$}
		\UIC{$B_{1}\otimes B_{2} \vdash C_{1}\otimes C_{2}$}
	\RightLabel{$(cut)$}
	\BIC{$A_{1}\otimes A_{2} \vdash C_{1}\otimes C_{2}$}
\DP
\quad
	\AXC	{$p(f_{1})$}
	\dottedLine
	\UIC{$A_{1}\vdash B_{1}$}
		\AXC	{$p(g_{1})$}
		\dottedLine
		\UIC{$B_{1}\vdash C_{1}$}
	\RightLabel{$(cut)$}
	\BIC{$A_{1} \vdash C_{1}$}
		\AXC{$p(f_{2})$}
		\dottedLine
		\UIC{$A_{2}\vdash B_{2}$}
			\AXC{$p(g_{2})$}
			\dottedLine
			\UIC{$B_{2}\vdash C_{2}$}
		\RightLabel{$(cut)$}
		\BIC{$A_{2} \vdash C_{2}$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A_{1},A_{2} \vdash C_{1}\otimes C_{2}$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A_{1}\otimes A_{2} \vdash C_{1}\otimes C_{2}$}		
\DP
}
\vspace{1mm}

\noindent
On a également l'égalité $id_{[A]\otimes[B]}=id_{[A]}\otimes id_{[B]}$ en raison de l'équivalence des preuves
\vspace{1.5mm}

\noindent
\resizebox{3cm}{!}{
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A\otimes B \vdash A\otimes B$}
	\DP
}
et
\resizebox{4cm}{!}{
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A \vdash A$}
		\AXC{}
		\RightLabel{$(id)$}
		\UIC{$B \vdash B$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A,B \vdash A\otimes B$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes B \vdash A\otimes B$}
	\DP
}
.
\vspace{3mm}

\


Ces propriétés signifient que l'opérateur $\otimes$ constitue un \textbf{bifoncteur} sur $\CProofs$, c'est-à-dire un foncteur de $\CProofs\times\CProofs$ vers $\CProofs$, avec les définitions suivantes.

\

\begin{df}
Soit $\Ccal$ et $\Dcal$ des catégories. Un \textbf{\emph{foncteur}} $F:\Ccal\longrightarrow\Dcal$ de $\Ccal$ vers $\Dcal$ consiste en une application des objets de $\Ccal$ vers les objets de $\Dcal$ et une application des morphismes de $\Ccal$ vers les morphismes de $\Dcal$, notées toutes deux $F$ par abus d'écriture, tel que
\begin{itemize}
\item
Pour tous objets $A$, $B$ et morphisme $f:A\to B$ de $\Ccal$, on a $F(f):F(A)\to F(B)$.
\item
Pour tous morphismes $f:A\to B$ et $g:B\to C$ de $\Ccal$, on a $F(g\circ f)=F(g)\circ F(f)$.
\item
Pour tout objet $A$ de $\Ccal$, on a $F(id_{A})=id_{F(A)}$.
\end{itemize}
\end{df}

\begin{df}
Soit $\Ccal$ et $\Dcal$ des catégories. On définit la \textbf{\emph{catégorie produit}} $\Ccal\times\Dcal$, où ${Obj(\Ccal\times\Dcal)}=Obj(\Ccal)\times Obj(\Dcal)$ et ${Hom_{(A,A')\to(B,B')}=Hom_{A\to B}(\Ccal)\times Hom_{A' \to B'}(\Dcal)}$.
\end{df}


%


\subsection{Catégorie monoïdale}

%On verra que munir la catégorie $\CProofs$ de $\otimes$ en fait une catégorie monoïdale. Mais pour définir ce qu'est une catégorie monoïdale, il faut plusieurs autres définitions sur les catégories.

La catégorie $\CProofs$ ayant été munie du bifoncteur $\otimes$, on se demande s'il s'agit d'une catégorie monoïdale, dont la définition est donnée après deux définitions auxiliaires.
%Une catégorie monoïdale peut se comprendre comme une catégorie munie d'une structure de monoïde qui a un ``bon comportement'' par rapport aux autres structures de la catégorie, le sens de ce ``bon comportement'' restant à définir.
%, qui s'appuie sur les deux définitions suivantes, est donnée ensuite.
%à leur suite.

\begin{df}
Soit une catégorie,
un morphisme $f:A\to B$ est un \textbf{\emph{isomorphisme}} s'il existe un morphisme $f^{-1}:B\to A$ tel que $f^{-1}\circ f=id_{A}$ et $f\circ f^{-1}=id_{B}$.
\end{df}


\begin{df}
Soit $\Ccal$ et $\Dcal$ des catégories. Soit $F,G:\Ccal\longrightarrow\Dcal$ des foncteurs de $\Ccal$ vers $\Dcal$. Une \textbf{\emph{transformation naturelle}} $\theta$ de $F$ vers $G$ est une famille $\big(\;\theta_{A} : F(A) \to G(A)\;\big)_{A\in Obj(\Ccal)}$ de morphismes de $\Dcal$, indexée par les objets de $\Ccal$, telle que le diagramme suivant commute dans $\Dcal$ pour tout morphisme $f:A\to B$ de $\Ccal$

{\centering
%\includegraphics[width=0.4\linewidth]{diagrammeNaturel}
\begin{tikzcd}
	F(A) \arrow[r,"\theta_{A}"] \arrow[d,"F(f)"] & G(A) \arrow[d,"G(f)"] \\
	F(B) \arrow[r,"\theta_{B}"] & G(B)
\end{tikzcd}

}
\noindent
On note $\theta : F \Rightarrow G$, voire $\theta : F \Rightarrow G : \Ccal \longrightarrow \Dcal$.
On appelle \emph{isomorphisme naturel} une transformation naturelle qui est une famille d'isomorphismes.
\end{df}





% def catégorie monoïdale
\begin{df}
Une \textbf{\emph{catégorie monoïdale}} est une catégorie $\Ccal$ munie d'un bifoncteur \\${\otimes:\Ccal\times\Ccal \longrightarrow \Ccal}$ (pour lequel on utilise une notation infixe) avec un objet particulier $e$, %tel que les morphismes suivants existent
%
%\begin{itemize}
%\item
%pour tous objets $A$, $B$, $C$, un isomorphisme naturel
%%\linebreak
%${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$ permettant de parler d'associativité ;
%\item
%pour tout objet $A$, des isomorphismes naturels ${\lambda_{A}:e\otimes A\to A}$ et ${\rho_{A}:A\otimes e\to A}$ %justifiant le nom d'\textbf{unité} pour $e$ ;
%permettant d'appeler $e$ l'objet \textbf{unité} ;
%\end{itemize}
telle qu'il existe des \textbf{isomorphismes naturels} :
\begin{itemize}
\item
${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$, permettant de parler d'associativité ;
\item
${\lambda_{A}:e\otimes A\to A}$ ;
\item
${\rho_{A}:A\otimes e\to A}$,
permettant avec le précédent d'appeler $e$ l'objet \textbf{unité} ;
\end{itemize}

\noindent
tels que les diagrammes suivants commutent pour tous objets $A$, $B$, $C$, $D$. 

%\includegraphics[width=\linewidth]{diagrammeMonoidalePenta}
%{\centering
%\includegraphics[width=0.5\linewidth]{diagrammeMonoidaleTri}
%			
%}
{\centering
\begin{tikzcd}
	& (A\otimes B)\otimes(C\otimes D) \arrow[rd,"\alpha_{A,B,C\otimes D}"] & \\
	((A\otimes B)\otimes C)\otimes D 
			\arrow[ru,"\alpha_{A\otimes B,C,D}"] 
			\arrow[d,"\alpha_{A,B,C}\otimes id_{D}"] 
		& & A\otimes(B\otimes(C\otimes D)) \\
	(A\otimes(B\otimes C)\otimes D 
			\arrow[rr,"\alpha_{A,B\otimes C,D}"]
		& & A\otimes((B\otimes C)\otimes D)
			\arrow[u,"id_{A}\otimes\alpha_{B,C,D}"]
\end{tikzcd}
\begin{tikzcd}
	(A\otimes e)\otimes B
			\arrow[rr,"\alpha_{A,e,B}"]
			\arrow[rd,"\rho_{A}\otimes id_{B}"]
		& & A\otimes(e\otimes B)
			\arrow[ld,"id_{A}\otimes\lambda_{B}"] \\
	& A\otimes B &
\end{tikzcd}
			
}
\noindent
Ces diagrammes sont appelés \emph{diagrammes pentagonal} et \emph{triangulaire}. Leur commutativité est la \emph{propriété de cohérence}.
%On a omis les indices de $\alpha$, $\lambda$, $\rho$ et écrit $A$ pour $id_{A}$ : par exemple, comprendre $\alpha\otimes D$ comme $\alpha_{A,B,C}\otimes id_{A}$.
%On omet souvent les indices de $\alpha$, $\lambda$, $\rho$ lorsqu'il n'y a pas d'ambiguïté. Ici, on écrit aussi $A$ pour $id_{A}$ : par exemple, comprendre $\alpha\otimes D$ comme ${\alpha_{A,B,C}\otimes id_{A}}$.

\vspace{2mm}

\noindent\textbf{Précision.} En fait la condition sur $\alpha$ est $\alpha : F \Rightarrow G :\Ccal\times\Ccal\times\Ccal \longrightarrow \Ccal$ avec $F:(x,y,z) \mapsto (x\otimes y)\otimes z$ et $G:(x,y,z) \mapsto x\otimes(y\otimes z)$ où $x$, $y$, $z$ sont trois objets ou trois morphismes de $\Ccal$. On écrit $\alpha_{A,B,C}$ pour $\alpha_{(A,B,C)}$. Les $\alpha_{A,B,C}$ sont bien des morphismes de $\Ccal$, qui est la catégorie d'arrivée de $F$ et $G$. On veut aussi ${\lambda : (e\otimes\bullet) \Rightarrow Id_{\Ccal} : \Ccal \longrightarrow \Ccal}$ où $(e\otimes\bullet) : \left\{
\begin{array}{l}
	A \mapsto e\otimes A \\
	f \mapsto id_{e}\otimes f
\end{array}
\right.$ et où $Id_{\Ccal}$ est le foncteur identité sur $\Ccal$. Il y a une condition similaire pour $\rho$.


\end{df}

Le nom de \emph{catégorie monoïdale} vient du fait que l'ensemble des objets de la catégorie (s'il s'agit bien d'un ensemble) muni de $\otimes$ devient un monoïde si on identifie la source et la cible de chaque isomorphisme donné par la définition.
\vspace{.5em}

%L'opérateur $\otimes$ qu'on a définit sur les dénotations de formules puis sur les dénotations de preuves constitue un \emph{bifoncteur} de la catégorie $\CProofs$, c'est-à-dire un foncteur $\otimes : \Ccal\times\Ccal \longrightarrow \Ccal$. $\Ccal$ munie de $\otimes$ est presque une \textbf{catégorie monoïdale} avec comme objet unité $[\1]$, la dénotation de la formule $\1$, élément neutre pour le connecteur $\otimes$.

On munit la catégorie $\CProofs$ du bifoncteur $\otimes$ et on choisit comme objet unité $[\1]$, la dénotation de la formule $\1$, élément neutre pour le connecteur $\otimes$ en logique linéaire intuitionniste. On obtient une \textbf{catégorie monoïdale}, ou presque une catégorie monoïdale, selon les choix effectués pour définir le procédé d'élimination de la coupure.
%La catégorie $\Ccal$ munie du bifoncteur $\otimes$ est presque une \textbf{catégorie monoïdale} avec comme objet unité $[\1]$, la dénotation de la formule $\1$, élément neutre pour le connecteur $\otimes$.
En effet, on peut définir des morphismes $\alpha_{A,B,C}$, $\lambda_{A}$, $\rho_{A}$ comme les dénotations des preuves suivantes\\

\def\alphaABC{
$\alpha_{A,B,C}$ :
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	  \AXC{}
	  \RightLabel{$(id)$}
	  \UIC{$B\vdash B$}
	    \AXC{}
	    \RightLabel{$(id)$}
	    \UIC{$C\vdash C$}
	  \RightLabel{$(\otimes R)$}
	  \BIC{$B,C \vdash B\otimes C$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A,B,C \vdash A\otimes(B\otimes C)$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes B,C \vdash A\otimes(B\otimes C)$}
	\RightLabel{$(\otimes L)$}
	\UIC{$(A\otimes B)\otimes C \vdash A\otimes(B\otimes C)$}
\DP
}

\def\lambdaA{
$\lambda_{A}$ :
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	\RightLabel{$(\1 L)$}
	\UIC{$\1,A\vdash A$}
	\RightLabel{$(\otimes L)$}
	\UIC{$\1\otimes A\vdash A$}
\DP
}

\def\rhoA{
$\rho_{A}$ : 
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$A\vdash A$}
	\RightLabel{$(\1 L)$}
	\UIC{$A,\1\vdash A$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes\1\vdash A$}
\DP
}

%\begin{tabular}{p{0.5\linewidth}p{0.5\linewidth}}
%	\multicolumn{2}{l}{\quad\; \alphaABC} \\ \\
%	\lambdaA & \rhoA
%\end{tabular}
% alpha : \qquad\qquad, 2 autres : \quad

\noindent
\resizebox{\linewidth}{!}{
\alphaABC \quad \lambdaA \quad \rhoA
}

\

\noindent
Il s'agit bien de transformations naturelles, et on a bien la propriété de cohérence (cf.~\cite{panorama}, chapitre~2).
%\idees{démonstrations, ou au moins explications, exemples de preuves équivalentes ?}
%
La définition d'une \emph{catégorie monoïdale} exige que ces morphismes $\alpha$, $\lambda$, $\rho$ soient des isomorphismes. On n'écrit plus les indices, qui sont toujours les mêmes. On peut définir des morphismes naturels $\bar\alpha_{A,B,C} : A\otimes(B\otimes C) \to (A\otimes B)\otimes C$ \,,\; $\bar\lambda_{A} : A \to e\otimes A$ \,,\; $\bar\rho_{A} : A \to A\otimes e$\,, par exemple $\bar\lambda_{A}$ est la dénotation de 
\resizebox{3cm}{!}{
	\AXC{}
	\RightLabel{$(\1 R)$}
	\UIC{$\vdash \1$}
		\AXC{}
		\RightLabel{$(id)$}
		\UIC{$A\vdash A$}
	\RightLabel{$(\otimes R)$}
	\BIC{$A\vdash \1\otimes A$}
\DP
}
. On pense naturellement à ces morphismes quand on cherche des inverses de $\alpha$, $\lambda$ et $\rho$. On a bien ${\lambda\circ\bar\lambda = id_{A}}$ et ${\rho\circ\bar\rho=id_{A}}$. En revanche, si on considère un p.é.c. ``strict'', on n'a aucune des égalités suivantes : ${\bar\lambda\circ\lambda = id_{e\circ A}}$\,,\; ${\bar\rho\circ\rho=id_{A\circ e}}$\,,\; ${\bar\alpha\circ\alpha=id_{(A\circ B)\circ C}}$\,,\; ${\alpha\circ\bar\alpha=id_{A\circ(B\circ C)}}$\,. Si on considère au contraire la version plus ``large'' du p.é.c., on a bien toutes ces égalités, donc $\alpha$, $\lambda$ et $\rho$ sont bien des isomorphismes. Les transformations autorisées qui ont été ajoutées au p.é.c. strict pour obtenir cette version large sont d'ailleurs expressément choisies pour satisfaire ces égalités. Dans la suite, on suppose toujours qu'on a choisi la version ``large'' du p.é.c., donc $\CProofs$ est bien une catégorie monoïdale.
%On obtient alors que $\CProofs$ est une \textbf{catégorie monoïdale}. Dans la suite, on se met toujours dans ce cas



%En revanche, si on considère un p.é.c. ``strict'', ces morphismes $\alpha$, $\lambda$, $\rho$ ne sont pas des isomorphismes. On n'écrit plus les indices, qui sont toujours les mêmes. On peut définir des morphismes naturels $\bar\alpha_{A,B,C} : A\circ(B\circ C) \to (A\circ B)\circ C$ \,,\; $\bar\lambda_{A} : A \to e\circ A$ \,,\; $\bar\rho_{A} : A \to A\circ e$\,, par exemple $\bar\lambda_{A}$ est la dénotation de 
%\resizebox{3cm}{!}{
%	\AXC{}
%	\RightLabel{$(\1 R)$}
%	\UIC{$\vdash \1$}
%		\AXC{}
%		\RightLabel{$(id)$}
%		\UIC{$A\vdash A$}
%	\RightLabel{$(\otimes R)$}
%	\BIC{$A\vdash \1\otimes A$}
%\DP
%}
%. On pense naturellement à ces morphismes quand on cherche des inverses de $\alpha$, $\lambda$ et $\rho$. On a bien ${\lambda\circ\bar\lambda = id_{A}}$ et ${\rho\circ\bar\rho=id_{A}}$, mais on n'a aucune des égalités suivantes : ${\bar\lambda\circ\lambda = id_{e\circ A}}$\,,\; ${\bar\rho\circ\rho=id_{A\circ e}}$\,,\; ${\bar\alpha\circ\alpha=id_{(A\circ B)\circ C}}$\,,\; ${\alpha\circ\bar\alpha=id_{A\circ(B\circ C)}}$\,.
%
%Cependant en ajoutant des transformations ...



%\def\noninverseUn{
%	\AXC{}
%	\RightLabel{$(id)$}
%	\UIC{$A\vdash A$}
%	\RightLabel{$(\1 L)$}
%	\UIC{$A,\1\vdash A$}
%	\RightLabel{$(\otimes L)$}
%	\UIC{$A\otimes\1\vdash A$}
%		\AXC{}
%		\RightLabel{$(id)$}
%		\UIC{$A\vdash A$}
%			\AXC{}
%			\RightLabel{$(\1 R)$}
%			\UIC{$\vdash \1$}
%		\RightLabel{$(\otimes R)$}
%		\BIC{$A\vdash A\otimes\1$}
%	\RightLabel{$(cut)$}
%	\BIC{$A\otimes\1\vdash A\otimes\1$}
%	\noLine
%	\UIC{}
%	\noLine
%	\UIC{$p_{1}$}
%\DP
%}
%\def\noninverseDeux{
%	\AXC{}
%	\RightLabel{$(id)$}
%	\UIC{$A\vdash A$}
%	\RightLabel{$(\1 L)$}
%	\UIC{$A,\1\vdash A$}
%	\RightLabel{$(\otimes L)$}
%	\UIC{$A\otimes\1\vdash A$}
%		\AXC{}
%		\RightLabel{$(\1 R)$}
%		\UIC{$\vdash \1$}
%	\RightLabel{$(\otimes R)$}
%	\BIC{$A\otimes\1\vdash A\otimes\1$}
%	\noLine
%	\UIC{}
%	\noLine
%	\UIC{$p_{2}$}
%\DP
%}
%\def\noninverseTrois{
%	\AXC{}
%	\RightLabel{$(id)$}
%	\UIC{$A\vdash A$}
%		\AXC{}
%		\RightLabel{$(id)$}
%		\UIC{$\1 \vdash \1$}
%	\RightLabel{$(\otimes R)$}
%	\BIC{$A,\1\vdash A\otimes\1$}
%	\RightLabel{$(\otimes L)$}
%	\UIC{$A\otimes\1\vdash A\otimes\1$}
%	\noLine
%	\UIC{}
%	\noLine
%	\UIC{$p_{3}$}
%\DP
%}
%\noindent
%\resizebox{\linewidth}{!}{
%%\begin{tabular}{ccc}
%%	\noninverseUn & \noninverseDeux & \noninverseTrois
%%\end{tabular}
%\noninverseUn \quad \noninverseDeux \quad \noninverseTrois
%}


%


\subsection{\'Echange et catégorie monoïdale symétrique}

\def\sizesymetrie{0.27\textwidth}
\begin{floatingfigure}[r]{\sizesymetrie}
\centering
\resizebox{\sizesymetrie}{!}{
	\AXC{}
	\RightLabel{$(id)$}
	\UIC{$B\vdash B$}
		\AXC{}
		\RightLabel{$(id)$}
		\UIC{$A\vdash A$}
	\RightLabel{$(\otimes R)$}
	\BIC{$B,A \vdash B\otimes A$}
	\RightLabel{$(exchange)$}
	\UIC{$A,B \vdash B\otimes A$}
	\RightLabel{$(\otimes L)$}
	\UIC{$A\otimes B \vdash B\otimes A$}
\DP
}
\\
\end{floatingfigure}

La logique linéaire est commutative : la règle d'échange ... permet de construire la preuve canonique de $A\otimes B \vdash B\otimes A$ ci-contre. On peut alors ajouter un nouvel isomorphisme naturel à $\CProofs$ pour en faire une catégorie monoïdale symétrique. Des variantes ``non commutatives'' ont été étudiées dans la littérature, où la règle d'échange est supprimée ou affaiblie. %Dans ce dernier cas, on peut parfois obtenir une catégorie monoïdale non pas symétrique, mais \emph{tressée}
Dans ce dernier cas, on peut parfois obtenir quand même une catégorie monoïdale tressée.

\vspace{3mm}
%\

\begin{df}
Soit $\Ccal$ une catégorie monoïdale, avec les notations précédentes. C'est une \textbf{\emph{catégorie monoïdale tressée}} s'il existe un \textbf{isomorphisme naturel} $\gamma_{A,B} : A\otimes B \to B\otimes A$ tel que les \emph{diagrammes hexagonaux} suivants commutent.
\vspace{1mm}

%\noindent
%\includegraphics[width=0.48\linewidth]{diagrammeHexa1}
%\quad
%\includegraphics[width=0.48\linewidth]{diagrammeHexa2}

{\centering
%\resizebox{0.7\linewidth}{!}{
\begin{tikzcd}
	& A\otimes(B\otimes C)
		\arrow[r,"\gamma_{A,B\otimes C}"]
	& (B\otimes C)\otimes A
		\arrow[rd,bend left,"\alpha_{B,C,A}"]
& \\
	(A\otimes B)\otimes C
		\arrow[ru,bend left,"\alpha_{A,B,C}"]
		\arrow[rd,bend right,"\gamma_{A,B}\otimes id_{C}"']	
	& & & B\otimes(C\otimes A)
\\
	& (B\otimes A)\otimes C
		\arrow[r,"\alpha_{B,A,C}"]
	& B\otimes(A\otimes C)
		\arrow[ru,bend right,"id_{B}\otimes\gamma_{A,C}"']
\end{tikzcd}
\quad

\noindent
\begin{tikzcd}
	& (A\otimes B)\otimes C
		\arrow[r,"\gamma_{A\otimes B,C}"]
	& C\otimes(A\otimes B)
		\arrow[rd,bend left,"\alpha^{-1}_{C,A,B}"]
& \\
	A\otimes(B\otimes C)
		\arrow[ru,bend left,"\alpha^{-1}_{A,B,C}"]
		\arrow[rd,bend right,"id_{A}\otimes\gamma_{B,C}"']	
	& & & (C\otimes A)\otimes B
\\
	& A\otimes(C\otimes B)
		\arrow[r,"\alpha^{-1}_{A,C,B}"]
	& (A\otimes C)\otimes B
		\arrow[ru,bend right,"\gamma_{C,A}\otimes id_{B}"']
\end{tikzcd}
%}

}
\end{df}



\begin{df}
Soit $\Ccal$ une catégorie monoïdale tressée, avec les notations précédentes. C'est une \textbf{\emph{catégorie monoïdale symétrique}} si pour tous objets $A$, $B$, on a $\gamma_{B,A}=\gamma_{A,B}^{-1}$. Dans ce cas, on n'a pas besoin de vérifier la commutativité du second diagramme de la définition précédente, qui est entraînée par la commutativité du premier diagramme appliquée à $\gamma_{B,A}$.
\end{df}

Dans $\Ccal$, on définit $\gamma_{A,B}$ comme la dénotation de la preuve canonique de $A\otimes B \vdash B\otimes A$ donnée précédemment. %Si on a assoupli l'équivalence d'élimination de la coupure pour que $\Ccal$ soit une catégorie monoïdale, on 
On obtient alors une \textbf{catégorie monoïdale symétrique}.
%\idees{vrai ? démonstrations, explications, exemples ?}


%



%\subsection{Implication linéaire et catégorie monoïdale fermée}
%
%Intéressons-nous maintenant à l'implication linéaire $\multimap$. Ce connecteur logique induit également un opérateur sur les dénotations de formules, qui a un sens en théorie des catégories.
%
%\begin{df}
%Soit $\Ccal$ une catégorie monoïdale, toujours avec les mêmes notations. Une \textbf{\emph{structure fermée à gauche}} sur $\Ccal$ consiste en un opérateur $\multimap$ sur les objets et pour tous objets $A$, $B$, un morphisme $eval_{A,B} : A\otimes(A\multimap B)\to B$, vérifiant la propriété d'universalité suivante : pour tout objet $X$ et tout morphisme $f:A\otimes X\to B$, il existe un unique morphisme $h:X\to A\multimap B$ tel que le diagramme suivant commute.
%
%\noindent
%{\centering
%%\includegraphics[width=4cm]{diagrammeFermeeGauche}
%\begin{tikzcd}
%	A\otimes X
%		\arrow[d,"id_{A}\otimes h"']
%		\arrow[rd,"f"]
%& \\
%	A\otimes(A\multimap B)
%		\arrow[r,"eval_{A,B}"']
%	& B
%\end{tikzcd}
%
%}
%On dit alors que $\Ccal$ est une \textbf{\emph{catégorie monoïdale fermée}}.
%\end{df}
%
%
%\def\sizesymetrie{0.3\textwidth}
%\begin{floatingfigure}[r]{\sizesymetrie}
%\centering
%\resizebox{\sizesymetrie}{!}{
%	\AXC{}
%	\RightLabel{$(id)$}
%	\UIC{$A\vdash A$}
%		\AXC{}
%		\RightLabel{$(id)$}
%		\UIC{$B\vdash B$}	
%	\RightLabel{$(\multimap L)$}
%	\BIC{$A,(A\multimap B) \vdash B$}
%	\RightLabel{$(\otimes L)$}
%	\UIC{$A\otimes(A\multimap B) \vdash B$}
%\DP
%}
%\\
%\end{floatingfigure}
%
%On munit $\CProofs$ de l'opérateur $\multimap$ sur les dénotations de formules correspondant à l'implication linéaire. On définit $eval_{A,B}$ comme la dénotation de la preuve ci-contre. On obtient alors une \textbf{catégorie monoïdale fermée}.
%%\idees{Est-ce vrai ? Est-ce qu'on peut expliquer facilement comment obtenir $h$ à partir de $f$ ?}
%

\subsection{Sur une généralisation à des preuves de n'importe quel séquent}

Dans la définition de $\CProofs$ donnée au début de cette section, on ne s'intéresse qu'aux preuves de séquents qui ont exactement une formule de chaque côté. Pour considérer la dénotation d'une preuve d'un séquent quelconque $\G\vdash D$ comme un morphisme de $\CProofs$, on a besoin d'associer une dénotation à la liste $\G$. Il serait naturel de choisir la dénotation de $G_{1}\otimes ... \otimes G_{k}$, mais ceci n'est pas une formule tant qu'on n'a pas ajouté de parenthèses. Lorsqu'on en ajoute, on obtient des formules qui sont certes équivalentes d'un point de vue logique, mais différentes d'un point de vue structurel, et les dénotations associées n'ont a priori pas de raison d'être égales. On peut résoudre ce problème grâce à une propriété de cohérence qui affirme que $\CProofs$ est équivalente, dans un certain sens qui serait trop long à définir ici, à une catégorie monoïdale symétrique stricte $\Ccal$ ; \emph{stricte} signifie que pour tous objets $A$, $B$, $C$ de $\Ccal$, avec les notations habituelles, on a $(A\otimes B)\otimes C=A\otimes(B\otimes C)$ et $e\otimes A=A=A\otimes e$.



%$\CProofs$, telle qu'elle a été définie au début de cette section, ne contient que des dénotations de preuves 
%
%Afin d'étendre ce formalisme à n'importe quel séquent intuitionniste $\G \vdash D$, on voudrait associer à $\G=G_{1}, ... , G_{k}$ la dénotation d'une formule de la forme $G_{1}\otimes ... \otimes G_{k}$, mais un problème d'associativité se pose : selon les endroits où on ajoute des parenthèses, on obtient des formules qui sont certes équivalentes, mais différentes d'un point de vue structurel, et les dénotations associées n'ont a priori pas de raison d'être égales. On peut résoudre ce problème grâce à une propriété de cohérence qui affirme que $\CProofs$ est équivalente, dans un certain sens qui serait trop long à définir ici, à une catégorie



%\idees{généralisation à n'importe quelle preuve de logique intuitionniste ?}

%%%
%\pagebreak

\section*{Conclusion}

Après avoir étudié le formalisme du calcul des séquents à travers différents exemples, on a transporté des propriétés sur un calcul des séquents pour ILL donné vers le langage mathématique des catégories. On peut encore interpréter le connecteur $\multimap$ d'ILL en terme de catégorie monoïdale fermée, et le connecteur $\&$ en terme de catégorie cartésienne fermée, et la modalité $!$ en terme d'adjonction entre une catégorie monoïdale symétrique fermée et une catégorie cartésienne ; mais étudier ces propriétés serait trop long pour le cadre de ce mémoire. Les catégories sont omniprésentes en mathématiques ; ces résultats permettent donc d'envisager des liens entre le calcul des séquents et des domaines qui semblent très différents au premier abord. Il y a une analogie forte entre la théorie de la preuve vue à travers le calcul des séquents et la théorie des n\oe uds ; les deux permettent notamment de construire de fa\c con similaire une catégorie libre, présentant de nombreuses propriétés, à partir d'une catégorie donnée (\cite{panorama}, pp.34-38).

%%%%%%%%%%%%%%

%\end{document}
\begin{comment}





\subsection{``Avec'' et produit cartésien}


\subsection{Catégorie libre}







\pagebreak





% def catégorie
\begin{df}
Une \textbf{\emph{catégorie}} consiste en une collection d'\emph{objets} et une collection de \emph{morphismes}, cette dernière munie d'une opérations binaire partielle $\circ$ appelée \emph{composition}, avec les propriétés suivantes.

\begin{itemize}

\item
\`A chaque morphisme $f$ est associé un couple d'objets $(A,B)$ ; on écrit $f:A\to B$.
On dit que $A\to B$ est le \emph{type} de $f$, et que $f$ est un morphisme \emph{de} $A$ \emph{vers} $B$, et encore que $A$ est le \emph{domaine} ou la \emph{source} de $f$, et $B$ le \emph{codomaine} ou la \emph{cible} de $f$.

\item
Pour tous objets $A$, $B$, $C$ et morphismes $f$, $g$ tels que $f:A\to B$ et $g:B\to C$, le morphisme $g\circ f$ existe et $g\circ f:A\to C$.

\item
\textbf{Identité.}
Pour tout objet $A$, il existe un morphisme particulier $id_{A}:A\to A$ appelé \emph{identitié sur $A$}, tel que 
%pour tous objet $B$ et morphismes $f:B\to A$ et $g:A\to B$, $id_{A}\circ f=f$ et $g\circ id_{A}=g$.
pour tout objet $B$, pour tout $f:B\to A$, $id_{A}\circ f=f$ et pour tout $g:A\to B$, $g\circ id_{A}=g$.

\item
\textbf{Associativité.}
Pour tous $f:A\to B$, $g:B\to C$, $h:C\to D$, on a $h\circ(g\circ f) = (h\circ g)\circ f$.

\end{itemize}
\end{df}

On désigne souvent une catégorie par la collection de ses objets.


% def catégorie monoïdale
\begin{df}
%Une \textbf{\emph{catégorie monoïdale}} est une catégorie $\C$ munie d'un bifoncteur ${\otimes:\C\times\C \longrightarrow \C}$ tel que
%
%\begin{itemize}
%%\item
%%pour tous objets $A$, $B$, $C$, il existe une isomorphisme naturel d'associativité
%%\linebreak
%%${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$ ;
%%\item
%%il existe un objet $e$, unitaire
%\item
%une \textbf{associativité} est fournie par un morphisme naturel pour tous objets $A$, $B$, $C$ : ${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$ ;
%\item
%il existe un objet $e$ \textbf{unitaire} grâce à des morphismes naturels pour tout objet $A$ : ${\lambda_{A}:e\otimes A\to A}$ et ${\rho_{A}:A\otimes e\to A}$.
% 
%\end{itemize}

Une \textbf{\emph{catégorie monoïdale}} est une catégorie $\Ccal$ munie d'un bifoncteur ${\otimes:\Ccal\times\Ccal \longrightarrow \Ccal}$ et d'un objet $e$, tel que les morphismes suivants existent

\begin{itemize}
\item
pour tous objets $A$, $B$, $C$, un isomorphisme
%\linebreak
${\alpha_{A,B,C} : (A\otimes B)\otimes C \to A\otimes(B\otimes C)}$ permettant de parler d'associativité ;
\item
pour tout objet $A$, des isomorphismes ${\lambda_{A}:e\otimes A\to A}$ et ${\rho_{A}:A\otimes e\to A}$ justifiant le nom d'\textbf{unité} pour $e$ ;
\end{itemize}

\noindent
et tel que les diagrammes suivants commutent pour tous objets $A$, $B$, $C$, $D$. On a omis les indices de $\alpha$, $\lambda$, $\rho$ et écrit $A$ pour $id_{A}$ : par exemple, comprendre $\alpha\otimes D$ comme $\alpha_{A,B,C}\otimes id_{A}$.

\includegraphics[width=\linewidth]{diagrammeMonoidalePenta}
\centering{\includegraphics[width=0.5\linewidth]{diagrammeMonoidaleTri}}

\end{df}





\pagebreak

\`A chaque preuve $\pi$ on associe une \emph{dénotation} $[\pi]$, qu'on veut invariante par élimination de la coupure.

\

Les objets sont les formules et les morphismes sont des dénotations de preuves. Les morphismes d'une formule $A$ vers une formule $B$ sont les dénotations des différentes preuves du séquent $A\vdash B$ (si le séquent n'est pas prouvable, il n'y en a pas).

Soit $A$, $B$, $C$ des formules, $\pi_{1}$ une preuve de $A\vdash B$ et $\pi_{2}$ une preuve de $B\vdash C$. On définit $[\pi_{2}]\circ[\pi_{1}]$ comme la dénotation de la preuve suivante de $A\vdash C$
\begin{prooftree}
	\AXC	{$\pi_{1}$}
	\noLine
	\UIC{$A\vdash B$}
	\AXC	{$\pi_{2}$}
	\noLine
	\UIC{$B\vdash C$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash C$}
\end{prooftree}

L'identité $id_{A}$ sur une formule $A$ est la dénotation de la preuve
{\centerAlignProof
	\AXC	{}
	\RightLabel{$(Id)$}
	\UIC{$A\vdash A$}
	\DP
}.
Soit $\pi$ une preuve de $A\vdash B$, l'élimination de la coupure transforme la preuve
\\
\begin{center}
	\AXC	{}
	\RightLabel{$(Id)$}
	\UIC{$A\vdash A$}
	\AXC	{$\pi$}
	\noLine
	\UIC{$A\vdash B$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash B$}
	\DP
\qquad\qquad en \qquad\qquad
	\AXC	{$\pi$}
	\noLine
	\UIC{$A\vdash B$}
	\DP
\end{center}

\noindent
donc on a bien $[\pi]\circ id_{A}=[\pi]$. De même pour $\pi$ une preuve de $B\vdash A$, on a $id_{A}\circ[\pi]=[\pi]$.

L'associativité vient de ce que les preuves
\\
\begin{center}
	\AXC	{$\pi_{1}$}
	\noLine
	\UIC{$A\vdash B$}
	\AXC	{$\pi_{2}$}
	\noLine
	\UIC{$B\vdash C$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash C$}
	\AXC	{$\pi_{3}$}
	\noLine
	\UIC{$C\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash D$}
	\DP
\quad et \quad
	\AXC	{$\pi_{1}$}
	\noLine
	\UIC{$A\vdash B$}
	\AXC	{$\pi_{2}$}
	\noLine
	\UIC{$B\vdash C$}
	\AXC	{$\pi_{3}$}
	\noLine
	\UIC{$C\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$B\vdash D$}
	\RightLabel{$(cut)$}
	\BIC{$A\vdash D$}
	\DP
\end{center}

\noindent
sont équivalentes à élimination de la coupure près.






\end{comment}

%\end{document}
